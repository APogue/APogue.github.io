---
layout: single
title: "The Pipeline"
permalink: /research/entry/
---

This page - How it works

[ Source Documents ]
    ‚Üì
(Incident scope boundaries)
    ‚Üì
[ AI System (Claude) ]
    ‚îú‚îÄ Applies codebook rules
    ‚îú‚îÄ Checks evidence thresholds
    ‚îú‚îÄ Extracts direct quotes
    ‚Üì
[ YAML Justification Block ]
    ‚îú‚îÄ Variable: actor_student = true
    ‚îú‚îÄ Justification: "DB-001: 'The student‚Ä¶'"
    ‚îú‚îÄ Source IDs: [DB-001]
    ‚Üì
[ Auditable Output ]
    ‚Üí Human-verifiable
    ‚Üí Structured + reproducible
    ‚Üí No inference beyond evidence

## System Architecture

The framework follows a structured pipeline:

The framework is built on a repeatable sequence ‚Äî from neutral incident discovery through structured source collection and rule-based coding, to pattern analysis and validation.

Each stage preserves a clear chain from source to conclusion, making findings reproducible and open to review.

Each step maintains:
- Clear audit trails
- Source-to-output traceability
- Reproducible methods
- Transparent decision logic

Annotate it with:

‚ÄúConstrained, not generative‚Äù

‚ÄúProtocol-bound decision logic‚Äù

‚ÄúEvidence-first processing‚Äù

‚ÄúSupports transparency + policy review‚Äù


The pipeline (at a glance)

Incident ‚Üí Source Trace ‚Üí Claude API ‚Üí Structured YAML ‚Üí Analysis

Incident: Defined via a neutral inclusion rule (Daily Bruin used for event discovery).

Source Trace: Link each data point to specific sources (admin statements, policy docs, media, org posts).

Claude API: Applies the codebook + protocols to produce justified variable assignments.

Structured YAML: One evidence file per incident with values + citations.

Analysis: Compare patterns (e.g., response disparities holding severity/visibility constant).

Core components

1) Inclusion & discovery

Neutral incident rule (scope, dates, campus affiliation)

Keyword search ‚Üí human screening (multiple incidents can map to one article and vice‚Äëversa)

2) Codebook & protocols

Clear variable definitions (binary, categorical, ordinal, quantitative, structured qualitative)

Enforcement logic prioritizes evidence standards over subjective judgement

3) Evidence standards

Granular citations: claims ‚Üí exact passages

Minimal sufficient evidence: enough to support, no over‚Äëanalysis

Auditability: each decision traceable end‚Äëto‚Äëend

4) AI + human review

Single‚Äëpass AI with attention refresh at checkpoints

Human verification for boundary cases and rubric drift





### üîç Explore the Pipeline  
**When the record stops, the questions start.**

---

#### **Step 1: Source-Based Structuring**
üìÑ **Source Documents ‚Üí üß† Structured Output**  
Claude codes each variable using official sources ‚Äî articles, policies, statements ‚Äî with traceable logic.

> *If a variable can‚Äôt be resolved from the record...*

---

#### **Step 2: Gap Detection**
‚ùì **Information Gap ‚Üí üîç Investigative Escalation**  
The system flags where the evidence ends ‚Äî missing follow-through, unclear enforcement, or institutional silence.

> *When documentation fails to answer a key question...*

---

#### **Step 3: External Outreach**
üìÇ **‚Üí FOIA / Public Records Requests**  
üí¨ **‚Üí Request for Institutional Comment**  
üé§ **‚Üí Interviews With Involved Parties**  

When documents go silent, the pipeline continues ‚Äî through structured outreach and testimony designed to fill the gap.



### 6. Technical Implementation
- **Pipeline:**  
  1. Input curated sources (DB articles, admin comms, policies)  
  2. Claude API applies codebook rules and extracts quotes  
  3. Outputs YAML with `value` + `justification` + `sources`  
  4. Automated and human validation  

- **Scale:** ~50+ incidents coded with 20+ variables each in ~10 minutes/incident (once sources are prepped)  
- **Interoperability:** YAML is human-readable and machine-parsable for analysis, visualization, or external audit


---

> **Every interview is tied to the question that prompted it. Every step stays on record.**


### How This Compares to Current Approaches
*Most systems force a trade-off between rigor, speed, and auditability ‚Äî this framework delivers all three.*

| Feature / Goal | **Manual Coding** (e.g., NVivo, Atlas.ti) | **Automated Detection** (e.g., GDELT, ACLED) | **This Framework** |
|----------------|------------------------------------------|----------------------------------------------|--------------------|
| **Evidence Link** | Quotes cited manually, not always consistent | Usually none ‚Äì relies on keyword or topic flags | **Every value tied to in-scope quotes** |
| **Rigor** | High, but slow and expensive | Low to moderate ‚Äì shallow context | **High ‚Äì enforces strict codebook rules** |
| **Scale** | ~10 incidents/month per researcher | Thousands/day | **50+ incidents in hours, with audit trail** |
| **Consistency** | Varies by coder | Consistent, but brittle to context changes | **Consistent + context-aware** |
| **Auditability** | Manual review of notes | Not audit-ready | **Fully audit-ready YAML outputs** |
| **Human Oversight** | Full | Minimal or none | **Targeted ‚Äì humans handle edge cases** |
| **Use of AI** | None or basic text search | Pattern detection, sentiment scoring | **Protocol-bound LLM as evidence auditor** |
| **Reproducibility** | Moderate ‚Äì depends on documentation | Low ‚Äì often proprietary | **High ‚Äì same inputs produce same outputs** |
| **Typical Output** | Narrative themes, coded spreadsheets | Event counts, maps | **Structured, machine-readable YAML + justifications** |



## Different standards 

codebook - definitions and values

codebook protocols - standards for evidence collection and thresholds for justification

neutrality standards - source inclusion 

## What is being studied 

Ahh, I see ‚Äî thanks for clarifying. The core point of your framework is:

Control for the ‚Äúobvious‚Äù explanatory factors (severity, visibility, policy violations, timing).

That way, if disparities remain, they‚Äôre less likely to be just correlation and more likely to reflect identity-based differences.

Then, as an add-on, you can explore whether those controlled variables do have predictive value in other contexts.


## Jargon

key factors - Severity, visibility, policy violations

core factors - group identity 

all *aspects* of an incident. this is what the AI searches for. there's an inclusion rule, there's an incident, there's an incident boundary. Then the AI searches and finds all *aspects* of the incident in order to assign *attributes*, these attributes are bla (see below)

response attribute is like timing, or policy whether formal rules were cited or enforced

Severity, visibility, policy violations, etc ‚Üí these are incident attributes (things about the event itself). They can plausibly be used as controls to explain variation in responses.

Timing of the response ‚Üí that‚Äôs already a response attribute. It‚Äôs part of the outcome, not an explanatory variable. Controlling for it or asking whether it predicts ‚Äúresponse type‚Äù is circular.

Temporal factors *(time between incident and administrative response)*

I developed the framework‚Äôs core components ‚Äî from codebook and protocol design to Claude API configuration and workflow optimization ‚Äî integrating engineering methods with social science research standards to ensure both technical precision and methodological rigor.

codebook is full of definitions, protocols are logic, api config are rules so that's api input side. api output side is reasoning not logic so as to not be messy with jargon. 

### Controlled Comparison

- Incident severity *(scale of harm or disruption involved)*
- Media visibility *(extent and reach of coverage)*
- Policy violations *(whether formal rules were clearly broken)*


