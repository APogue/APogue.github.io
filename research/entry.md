---
layout: single
title: "The Pipeline"
permalink: /research/entry/
---

# Research Pipeline

This project evaluates how UCLA responded to protest-related incidents and whether those responses aligned with policy and legal obligations.

### Evidence Sources
1. **Public Administrative Communication**  
   - Chancellor and administrative statements, press releases, campuswide emails.  
   - Purpose: Measure tone, framing, and timing of responses.  

2. **Internal Communication**  
   - External communications to administrators, internal administrator emails, outgoing administrator communications.  
   - Purpose: Compare what administrators knew vs. what was stated publically. Compare how administrators planned to respond to incidents vs. what administrative actions occurred.  

3. **Independent/3rd Party Reports**  
   - Commissioned investigations, task force reviews, consultant reports.  
   - Purpose: Establish how risks and responses were externally evaluated.  

4. **Policy & Legal Framework**  
   - UC policy (time, place, manner rules), First Amendment principles, Title VI obligations, USAC rules.  
   - Purpose: Benchmark UCLA’s obligations against actual practice.  

5. **Police Reports**  
   - UCPD incident reports, crime logs, arrest data.  
   - Purpose: Identify how incidents were recorded, investigated, or closed.  

6. **First-Hand Accounts (Critical Evidence Gap Filler)**  
   - Because PRA/FOIA requests can produce limited records, first-hand accounts are essential to establish what reports were filed and how UCLA responded.  
   - Scope: Not general perceptions of climate, but **factual details of process**:  
     - Who filed reports (EDI, Title VI, Dean of Students, UCPD).  
     - What acknowledgment, follow-up, or closure notices were received.  
     - What (if any) protective or remedial measures UCLA offered.  
   - Purpose: To verify whether UCLA fulfilled its Title VI duty to alleviate a hostile environment once on notice of identity-based exclusion or harassment.  

### Analytical Questions
- How were administrative responses correlated with **group identity, media visibility, and severity of incidents**?  
- Did UCLA consistenly address the **hostile environment** obligations under Title VI?
- Did failures to address hostile environment obligations lead to greater hostilities? E.g. did UCLA act when given constructive notice, did UCLA assess 

kaplan indicates premeditated and coordited threat potential (elevated threat)
kaplan indicates SAMS were not qualified to handle the situation
kaplan indicates threats could escalate to physical violence
kaplan indicates people pay attention to social media (take the views number as the baseline)
kaplan indicates the greater community is aware of what is going on (tons of death threats to the department caused them to move the meeting to zoom)
kaplan indicates that SAMS take true threats and violence as a free speech issue
internal emails indicate that the same incident was framed as a campus climate issue between students

Did UCLA leave the police report in the hands of the student?
If the masked perps were not identified, did UCLA publicly cite violations of the law or university policy or use language intended to threaten institutional preparedness for anyone planning to come to campus and harrass students? 
Did UCLA take any steps to try to alleviate a potential hostile environment for the students after the report was submitted?
Did UCLA inform the police department at any time during the year that premeditated, coordinated, non-affiliate harassment presented an elevated threat?

when confronted with the matter directly (walk out), block made a statement the next day that indicated deliberate indifference to the issue. It's at this point that when given sevaral opportunities to address one instance of severe harrassment, the school opted not to. They opted to site general safety (beck or someone wrote a statement about keeping campus safe at around the same time). 

Then treat the encampment issues as several events. (myers cited an event)

Was UCLA aware of the blockading for days (internal emails)
Was UCLA aware of the attack on the encampment the night before
Was UCLA aware of the massive social media response to the blockading
Did UCLA decide to use emotional framing in a social media post and public facing statment the evening following the mob harrassment of the encampment?
Did UCLA choose to use emotional framing in any other scenario concerning other vulnerable student groups?
Was UCLA aware the students were not leaving given they stated they would not go in the DB? (constructive notice)
Was UCLA aware of the heightened threat possibility given the harrassment that had occured thus far and the past incident?
Did UCLA inform UCPD to be prepared for a coordinated attack?
Did UCLA use any language as a deterrent from either harrassment or violence to try to mitigate hostilities up to that point?
If Block stated that the notice to clear the encampment was a security measure, why didn't they shut the encampment down?
Why didn't they shut the encampment down if it was behaving unlawfully and students were under threat?
He both removed the barriers (said in his statement) and declared the encampment unauthorized making it extremely unclear what expectations for students were (was the encampment still technically acting unlawfully, if so what was the basis for unauthorization?), regardless, they said they would not leave. that puts the responsibility on the university to keep them safe. and at that point anything they could have done would not have been enough because they already underassessed the threat potential. 





- How did failures in communication, deterrence, or preparedness contribute to escalation?  

The pipeline is designed to separate factual findings from narrative framing, ensuring that evidence is evaluated before interpretation.













<!-- 
This page - How it works

[ Source Documents ]
    ↓
(Incident scope boundaries)
    ↓
[ AI System (Claude) ]
    ├─ Applies codebook rules
    ├─ Checks evidence thresholds
    ├─ Extracts direct quotes
    ↓
[ YAML Justification Block ]
    ├─ Variable: actor_student = true
    ├─ Justification: "DB-001: 'The student…'"
    ├─ Source IDs: [DB-001]
    ↓
[ Auditable Output ]
    → Human-verifiable
    → Structured + reproducible
    → No inference beyond evidence

## System Architecture

The framework follows a structured pipeline:

The framework is built on a repeatable sequence — from neutral incident discovery through structured source collection and rule-based coding, to pattern analysis and validation.

Each stage preserves a clear chain from source to conclusion, making findings reproducible and open to review.

Each step maintains:
- Clear audit trails
- Source-to-output traceability
- Reproducible methods
- Transparent decision logic

Annotate it with:

“Constrained, not generative”

“Protocol-bound decision logic”

“Evidence-first processing”

“Supports transparency + policy review”


The pipeline (at a glance)

Incident → Source Trace → Claude API → Structured YAML → Analysis

Incident: Defined via a neutral inclusion rule (Daily Bruin used for event discovery).

Source Trace: Link each data point to specific sources (admin statements, policy docs, media, org posts).

Claude API: Applies the codebook + protocols to produce justified variable assignments.

Structured YAML: One evidence file per incident with values + citations.

Analysis: Compare patterns (e.g., response disparities holding severity/visibility constant).

Core components

1) Inclusion & discovery

Neutral incident rule (scope, dates, campus affiliation)

Keyword search → human screening (multiple incidents can map to one article and vice‑versa)

2) Codebook & protocols

Clear variable definitions (binary, categorical, ordinal, quantitative, structured qualitative)

Enforcement logic prioritizes evidence standards over subjective judgement

3) Evidence standards

Granular citations: claims → exact passages

Minimal sufficient evidence: enough to support, no over‑analysis

Auditability: each decision traceable end‑to‑end

4) AI + human review

Single‑pass AI with attention refresh at checkpoints

Human verification for boundary cases and rubric drift





### 🔍 Explore the Pipeline  
**When the record stops, the questions start.**

---

#### **Step 1: Source-Based Structuring**
📄 **Source Documents → 🧠 Structured Output**  
Claude codes each variable using official sources — articles, policies, statements — with traceable logic.

> *If a variable can’t be resolved from the record...*

---

#### **Step 2: Gap Detection**
❓ **Information Gap → 🔍 Investigative Escalation**  
The system flags where the evidence ends — missing follow-through, unclear enforcement, or institutional silence.

> *When documentation fails to answer a key question...*

---

#### **Step 3: External Outreach**
📂 **→ FOIA / Public Records Requests**  
💬 **→ Request for Institutional Comment**  
🎤 **→ Interviews With Involved Parties**  

When documents go silent, the pipeline continues — through structured outreach and testimony designed to fill the gap.



### 6. Technical Implementation
- **Pipeline:**  
  1. Input curated sources (DB articles, admin comms, policies)  
  2. Claude API applies codebook rules and extracts quotes  
  3. Outputs YAML with `value` + `justification` + `sources`  
  4. Automated and human validation  

- **Scale:** ~50+ incidents coded with 20+ variables each in ~10 minutes/incident (once sources are prepped)  
- **Interoperability:** YAML is human-readable and machine-parsable for analysis, visualization, or external audit


---

> **Every interview is tied to the question that prompted it. Every step stays on record.**


### How This Compares to Current Approaches
*Most systems force a trade-off between rigor, speed, and auditability — this framework delivers all three.*

| Feature / Goal | **Manual Coding** (e.g., NVivo, Atlas.ti) | **Automated Detection** (e.g., GDELT, ACLED) | **This Framework** |
|----------------|------------------------------------------|----------------------------------------------|--------------------|
| **Evidence Link** | Quotes cited manually, not always consistent | Usually none – relies on keyword or topic flags | **Every value tied to in-scope quotes** |
| **Rigor** | High, but slow and expensive | Low to moderate – shallow context | **High – enforces strict codebook rules** |
| **Scale** | ~10 incidents/month per researcher | Thousands/day | **50+ incidents in hours, with audit trail** |
| **Consistency** | Varies by coder | Consistent, but brittle to context changes | **Consistent + context-aware** |
| **Auditability** | Manual review of notes | Not audit-ready | **Fully audit-ready YAML outputs** |
| **Human Oversight** | Full | Minimal or none | **Targeted – humans handle edge cases** |
| **Use of AI** | None or basic text search | Pattern detection, sentiment scoring | **Protocol-bound LLM as evidence auditor** |
| **Reproducibility** | Moderate – depends on documentation | Low – often proprietary | **High – same inputs produce same outputs** |
| **Typical Output** | Narrative themes, coded spreadsheets | Event counts, maps | **Structured, machine-readable YAML + justifications** |



## Different standards 

codebook - definitions and values

codebook protocols - standards for evidence collection and thresholds for justification

neutrality standards - source inclusion 

## What is being studied 

Ahh, I see — thanks for clarifying. The core point of your framework is:

Control for the “obvious” explanatory factors (severity, visibility, policy violations, timing).

That way, if disparities remain, they’re less likely to be just correlation and more likely to reflect identity-based differences.

Then, as an add-on, you can explore whether those controlled variables do have predictive value in other contexts.


## Jargon

codebook - class definitions

codebook protocols - how to classify incident aspects

evidentiary standards - threshold minimums for certain classifications (might fall under protocols)

key factors - Severity, visibility, policy violations

core factors - group identity 

all *aspects* of an incident. this is what the AI searches for. there's an inclusion rule, there's an incident, there's an incident boundary. Then the AI searches and finds all *aspects* of the incident in order to assign *attributes*, these attributes are bla (see below)

response attribute is like timing, or policy whether formal rules were cited or enforced

Severity, visibility, policy violations, etc → these are incident attributes (things about the event itself). They can plausibly be used as controls to explain variation in responses.

Timing of the response → that’s already a response attribute. It’s part of the outcome, not an explanatory variable. Controlling for it or asking whether it predicts “response type” is circular.

Temporal factors *(time between incident and administrative response)*

I developed the framework’s core components — from codebook and protocol design to Claude API configuration and workflow optimization — integrating engineering methods with social science research standards to ensure both technical precision and methodological rigor.

codebook is full of definitions, protocols are logic, api config are rules so that's api input side. api output side is reasoning not logic so as to not be messy with jargon. 

### Controlled Comparison

- Incident severity *(scale of harm or disruption involved)*
- Media visibility *(extent and reach of coverage)*
- Policy violations *(whether formal rules were clearly broken)*

-->

