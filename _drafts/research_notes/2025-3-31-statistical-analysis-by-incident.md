---
layout: outline
title: "Statistical analysis by incident"
published: true
author: Alexie Pogue
date: 2025-3-31 6:41 PM
updated_date: 2025-4-3 9:45 PM
---

I must be unbiased in this study 

### Material of interest

- lawsuits
- task force reports 
- surveys
- studies
- newsroom statements
- chancellor statements
- UCOP statements
- Daily Bruin articles 
- USAC resolutions or statements 
- Reddit
- social media posts 
	- from the administration 
	- from student organizations 

---

### Create a grounded, source-agnostic incident index 

Rather than starting with lawsuits or admin comms, I'll build my incident list from the most neutral, time-ordered source available:

Best options:

- Daily Bruin: Covers both sides, timestamps all events, and youâ€™ve already tested it for keyword differences.

Build a master list of incidents first, based only on what happened and when, without regard to group or admin reaction. Only after that do you code for Target_Group, Response, etc.


### Define an incident inclusion rule (must stick to it)

To avoid subjective filtering, create a rule for what counts as an incident 

For example:

â€œAny campus-affiliated event between Sept 2023 and June 2024 where:
(a) identity-based harassment, exclusion, or violence was alleged;
(b) symbolic or physical protest occurred on university grounds; or
(c) TPM violations or safety breakdowns were documented.â€

Apply that rule consistently. Some will involve Jewish students, some Muslim/Pali â€” the rule determines inclusion, not identity or severity.

### Tag, no filters by source 

I might still find incidents via lawsuits, surveys, etc., but I won't use those to determine which ones get included. 

Instead:

- If lawsuit mentions incident, admin responded â†’ tag that later
- But even if they didn't â†’ it still goes in

This avoids confirmation bias.

### Supplement for representation, don't balance artificially 

I donâ€™t have to have an equal number of Jewish and Palestinian incidents â€” real-world bias might mean there are more of one kind of incident and fewer of another. What's important is:

- Your inclusion rule is neutral
- Your coding is consistent
- Your analysis accounts for coverage/severity gaps (e.g., via controls like Media_Coverage_Level)
	- to "account for gaps," adjust for confounding variables like Severity_Score, Media_Coverage_Level, Policy_Broken

â€œDo incidents involving Palestinian students receive less administrative response than those involving Jewish students even when the severity and visibility are the same?â€

â€œAmong incidents with equal media coverage and equal severity, does the administration still respond differently depending on which group is affected?â€

If I find a consistent disparity after controlling for those other factors â†’ thatâ€™s evidence of bias.

â€œAccount forâ€ = donâ€™t ignore the fact that Jewish incidents might look different on paper â€” control for that so your findings reflect bias, not circumstance.

But,â€œIf both the media and the administration are biased â€” how can I isolate administrative bias without just proving they respond to media pressure?â€

If admin responses correlate with media coverage, it could mean:

- Admins are just responding to whatâ€™s visible (neutral media-following behavior), or
- Admins are biased like the media (shared worldview), or
- Admins are biased independent of the media (selective attention within similarly covered events)

Youâ€™re testing whether the pattern of behavior is systematically unequal even after accounting for neutral factors like visibility or severity.

ğŸ§± Use Media Coverage as a Covariate, Not a Shield

Youâ€™re not saying media coverage justifies admin action. Youâ€™re testing whether media coverage explains the action â€” and whether it explains all of it.

- If it does: the admin is reactive.
- If it doesnâ€™t: the admin is selectively reactive â€” thatâ€™s bias.

ğŸ” Lean Into Transparency

Youâ€™ll strengthen your work by saying:

â€œMedia coverage does influence administrative behavior â€” but we tested whether that explains all the disparity, and it doesnâ€™t.â€

- Youâ€™re not denying correlation.
- Youâ€™re saying correlation alone doesnâ€™t account for the patterned disparity.
- Be clear that youâ€™re not proving causation â€” youâ€™re showing systematic disparities in treatment that canâ€™t be explained by neutral factors alone

---

### Methodology 

#### What is this research

In research terms, youâ€™re working in the realm of:

- Qualitative Comparative Analysis (QCA)
- Case-based bias detection
- Institutional behavior analysis
- Policy response comparison

Given your project is:

- Investigating bias using incident-level comparison
- With controlled coding of metadata
- And qualitative variables like tone, latency, and severity

Using categories like Yes/Strong for bias detection patterns is a well-established approach in policy analysis, institutional review, and legal-impact studies.

As long as:

- You define your coding scheme explicitly
- You apply it consistently
- You distinguish it from statistical inference

Structured Comparison, Not Inference

Your work involves:

- Building a complete dataset of known events, not a sample
- Coding for descriptive and categorical patterns (e.g., who admin responded to)
- Looking for systematic disparities, not estimating probabilities

You're not trying to say:

"This proves, with p < 0.05, that UCLA is biased."

You're saying:

â€œAcross all comparable incidents documented via a neutral source, there is a consistent pattern of unequal treatment that is not explained by severity, visibility, or legality.â€

Thatâ€™s qualitative comparative research, not statistical inference â€” and thatâ€™s totally valid in policy, legal, and bias studies.

So I can't extrapolate, I can't say the university is biased. I can say the administration is biased â€” within the scope of the dataset and under clearly defined parameters.

âœ… Hereâ€™s How You Make That Claim Rigorously

â€œBased on a comprehensive review of all incidents reported by The Daily Bruin between [years], and applying a consistent inclusion rule and coding scheme, UCLAâ€™s administration demonstrated a pattern of differential response depending on the identity group affected â€” even when controlling for severity, visibility, and policy violations.â€

This is a strong claim:

- Youâ€™re not generalizing beyond what you observed
- Youâ€™re not estimating the rate of bias in the universe of all possible incidents
- Youâ€™re identifying documented disparity within a defined corpus

ğŸ“ What You Can Say

âœ… â€œThere is clear evidence of disparate treatment within documented cases.â€

âœ… â€œPatterns of administrative response show group-based asymmetry.â€

âœ… â€œEven among incidents of similar severity and visibility, the university responded differently depending on the group affected.â€

âŒ What You Shouldnâ€™t Say (without statistical inference)

âŒ â€œUCLA is universally biased across all incidents involving these communities.â€

âŒ â€œThis proves the administration is institutionally antisemitic/Islamophobic.â€

âŒ â€œX% of the time, they behave in a biased way.â€

ğŸ§  So Yes â€” Youâ€™re Claiming Conditional Bias

UCLA exhibited bias under the following conditions:

- Incident documented by Daily Bruin
- Within the timeframe studied
- Where severity, visibility, and legality were held constant or coded

And thatâ€™s how most serious bias studies work 

---

### Defend the methodology: DB is my sole incident source

â€œThe Daily Bruin is the most comprehensive and continuously maintained public record of UCLA campus life from the student perspective.â€

âœ… 1. Itâ€™s UCLAâ€™s Student Paper of Record

- It's the only consistent, searchable, longitudinal source of campus events, student perspectives, and protest coverage over a decade.
- Administrators and students alike rely on it to understand campus dynamics.
- Unlike external media or admin comms, DB offers event-driven reporting, not top-down messaging.

â€œArticles were used to identify and time-stamp relevant incidents. No editorial interpretation from the Bruin was included in bias analysis.â€

âœ… 2. Youâ€™re Not Using It for Interpretation â€” Just Event Discovery

- You are not coding bias in their language, youâ€™re using it to identify that an incident happened on a given date, place, and with which parties involved.
- You then cross-reference the incident with admin responses, lawsuits, social media, etc.

âœ… 3. You Apply a Consistent Keyword Search and Inclusion Rule

This makes your methodology replicable and objective:

- You donâ€™t cherry-pick headlines â€” you define clear inclusion rules and search terms used across all years.
- If someone else repeated your method with the same archive, theyâ€™d get a similar incident list.

âœ… 4. You Cross-Check Admin Response Using Adminâ€™s Own Words

- The potential bias of DB is irrelevant to your main dependent variables â€” youâ€™re testing how UCLA responded, not what DB said about it.
- That makes the Bruin a cleaner baseline than admin comms, which already encode institutional motives.

ğŸ¯ Why I'm Right to Stick with the Daily Bruin

âœ… 1. Social media is not objective or complete

- Posts are ephemeral â€” deleted, edited, censored, or algorithmically buried.
- Thereâ€™s no comprehensive archive, no formal standards for coverage, no consistency in what's documented.
- It's impossible to apply a replicable incident-selection method on social media â€” you'd be relying on anecdotal, unverifiable, or engagement-biased content.

âœ… 2. DB gives you structure and timestamped reporting

- Searchable and chronologically structured
- Consistent over time (you wonâ€™t get a gap in 2017 because Instagram changed its algorithm)
- Credible enough that admin, students, and media all treat it as a public record
- You can cite specific articles, publication dates, and quotable content to ground your timeline

âœ… 3. Methodology matters more than exhaustiveness

"This isnâ€™t about cutting corners â€” itâ€™s about minimizing noise so I can precisely measure the administrationâ€™s behavior against an externally grounded record of student life."

Trying to include every mention from social media would make your study:

- Messy (inconsistent sourcing, unverifiable dates, fragmentary incidents)
- Non-replicable (no one can run the same scrape and get your dataset)
- Vulnerable to claims of cherry-picking or selective inclusion

By contrast, sticking to DB ensures:

- Clarity and transparency
- You can say: â€œEvery incident in this study was selected based on a consistent archival method from a single longitudinal source.â€

--- 

### Defend the methodology: My choices for the dependent variables sources 

âœ… 1. Define a fixed set of DV source types up front

For structure and transparency 

Examples 

- Admin emails and public statements
- Task force or working group reports
- UCOP or Chancellor-level communications
- Lawsuit filings (only if they contain direct admin quotes or responses)

âœ… Stick to these consistently â€” no ad hoc additions later unless logged as a scope expansion.

âœ… 2. Apply all relevant DV source types to every incident

Donâ€™t pick and choose based on which sources are available or interesting.

Instead:

- Create a source check template per incident
- Ask: Did this incident produce any content from each DV source type?

If yes â†’ log and code it

If no â†’ mark as â€œnone observedâ€ or â€œno public responseâ€

This way youâ€™re not selecting responses â€” you're checking whether they exist, from a consistent list.

âœ… 3. Make your coding definitions as replicable as your inclusion rule

Field: Admin_Response

Definition:

- Yes = A public statement, policy action, or formal email that references the incident
- No = No identifiable public communication or response referencing the incident

ğŸ¯ **Dependent Variables (DVs) Examples**

These measure administrative behavior â€” the outcomes you're testing for bias.

- **All of these should be structured (rule-based)**
- You want them clear, consistent, and comparable

Examples (Structured):

- Admin_Response (Yes/No)
- Tone_of_Response (Neutral, Punitive, Conciliatory...)
- Latency (in days)
- Follow_Up_Action (Yes/No)

---
### Defend the methodology: My choices for ğŸ§© Independent / Control Variable Sources

âœ… 1. Define a fixed set of orgs up front

But donâ€™t aim for perfect symmetry â€” aim for methodological neutrality.

Choose orgs based on:

- Their visibility in DB-covered incidents (mention frequency)
- Whether they are consistently involved as actors, targets, or organizers
- Not on identity balancing (i.e., youâ€™re not required to have equal numbers of â€œeach sideâ€)

ğŸ§  So yes â€” your approach might look like:

Included student orgs (based on incident involvement and visibility):

- SJP (Students for Justice in Palestine)
- JVP (Jewish Voice for Peace)
- Hillel at UCLA/Dan Brown (were formally tied to incidents)
- Chabad at UCLA
- Bruins for Israel
- CAC

Other relevant docs
- Student government statements 
- Lawsuit filings
- OCR investigations 

You can say:
â€œThese orgs were selected based on their repeated appearance in Daily Bruin coverage of relevant incidents between [dates].â€

âœ… This makes your selection criteria **visibility-based, not identity-based.**

âœ… 2. What if the visibility is lopsided?

Thatâ€™s okay â€” and in fact, itâ€™s data. If certain orgs are more active, more covered, or more responded to, thatâ€™s part of the story.

The key is:

- You didn't exclude others arbitrarily
- You defined your selection criteria before DV coding
- You didnâ€™t cherry-pick based on tone or outcome

ğŸ§± Final structure:

Define a fixed list of org accounts you'll monitor

- Use them only for student tone, visibility, and framing
- Log all findings or absences per incident
- Document your org selection method clearly

Thatâ€™s how you avoid both cherry-picking and artificial balancing â€” youâ€™re just tracking who actually showed up.

So you're not treating student org posts as DVs â€” you're using them to code:

- Protest_Intensity
- Student_Tone
- Visibility_Level

These are for capturing nuance. You want student_tone to be separate from incident_severity and media_coverage_level because it doesn't necessarily correlate with those things. 

They become independent or control variables to isolate bias in admin behavior.

For the Kaplan example:

- SJP is mentioned â†’ you check SJPâ€™s channels for protest framing, intent, student tone

- Hillel responds publicly â†’ log that response as:

	- Visibility signal (it adds coverage)
	- Possibly affecting admin response (indirectly â€” e.g., framing it as antisemitic)
	- Contextual framing (e.g., competing narratives)

Youâ€™re not â€œincluding Hillelâ€ as a party to the incident unless they were directly involved.

Youâ€™re just logging that they responded â€” and that may factor into things like:

- Media_Coverage_Level
- Narrative_Pressure
- Admin_Response_Justification

Media_Coverage_Level:
- Low = Daily Bruin only, minimal social media traction
- Moderate = DB + student org posts or local blog mention (e.g., LAist)
- High = Coverage by external mainstream media (e.g., LA Times, CNN), or wide viral spread


#### ğŸ§© Independent / Control Variables Examples

These help you explain or isolate what might influence the DVs.

They can be either:

- **Structured** (quantifiable, categorical)
- **Qualitative** (interpretive, rubric-based)

**Examples (Structured):**

- `Severity_Score` (Low / Moderate / High)
- `Target_Group`
- `Media_Coverage_Level`
- `Student_Action_Type` (e.g., protest, statement, disruption)

**Examples (Qualitative):**

- `Student_Tone` (based on org posts or chants) --- not sure about this, should be consistent with DVs
- `Narrative_Framing` (e.g., "security risk" vs. "civil rights")
- `Competing_Claims` (whether multiple orgs weighed in publicly)


ğŸ§  Why this matters:

You're capturing **the ecosystem around the incident** â€” who was involved, who amplified, and who shaped **admin perception.**
But your unit of analysis stays the same: the incident, not the org.

---
### List organization

#### What is a Master Incident List?
Your main dataset â€” one row per incident.

Includes:

- Incident_ID
- Structured fields (date, severity, target group, etc.)
- Qualitative fields (tone, latency)
- Source_IDs â†’ links to entries in the Source Appendix

#### ğŸ“‹ Sample Master Incident List

| Incident_ID | Date       | Target_Group | Severity_Score | Admin_Response | Tone_of_Response | Media_Coverage_Level | Source_IDs                 |
|-------------|------------|--------------|----------------|----------------|------------------|-----------------------|----------------------------|
| INC-001     | 2024-04-30 | Palestinian  | High           | Yes            | Punitive         | High                  | DB-045, ADM-014, HIL-003   |
| INC-002     | 2024-05-03 | Jewish       | Moderate       | Yes            | Conciliatory     | Moderate              | DB-047, ADM-017            |
| INC-003     | 2024-05-05 | Palestinian  | Low            | No             | â€”                | Low                   | DB-048                     |
| INC-004     | 2024-05-10 | Jewish       | High           | Yes            | Neutral          | High                  | DB-050, ADM-020, HIL-004   |


#### What is the Source Appendix?

Itâ€™s a master list of all individual sources, regardless of incident.
Each row = one source, with a unique Source_ID.

So yes â€” a source (e.g., ADM-014) can be related to multiple incidents if relevant to each.

ğŸ“ **Sample Source Appendix Structure**

| Source_ID | Type         | Title                            | Date       | Use (Incidents)     |
|-----------|--------------|----------------------------------|------------|---------------------|
| DB-045    | DB Article   | â€œPolice Remove Protestersâ€¦â€      | 2024-05-01 | INC-004, INC-006     |
| ADM-014   | Admin Email  | Chancellor's Campus Update       | 2024-05-02 | INC-004              |
| HIL-003   | Hillel IG    | â€œWe are alarmed by recentâ€¦â€      | 2024-05-02 | INC-004              |



#### ğŸ“š What is a Source Library / Repository?

This is your folder of saved source materials.

Itâ€™s not a table â€” itâ€™s where you store the actual documents or links (PDFs, screenshots, archived web pages).

Each file or link should be named by its Source_ID, so DB-045.pdf or ADM-014.txt matches the entries in your Source Appendix.

--- 

### Possible outcome of study

That possibility is exactly what makes your research credible.

If you go through this process honestly and rigorously, and come out with:

- No consistent disparity in administrative response
- No bias when severity and visibility are held constant
- No group-based patterns in tone, latency, or follow-up

â€¦then you can confidently say:

â€œThis study found no evidence of systematic administrative bias in UCLAâ€™s public response to documented incidents, based on a consistent, neutral inclusion rule.â€

That is still a valid and valuable outcome.

ğŸ§­ But Hereâ€™s the Reality:

Given what youâ€™ve already seen â€” lawsuits, student testimony, visibility patterns, unequal framing â€” itâ€™s unlikely youâ€™ll come out with nothing.

You might find:

- Bias by omission (e.g., silence on Muslim studentsâ€™ complaints)
- Bias by framing (e.g., invoking â€œJewish fearâ€ to justify crackdowns)
- Inconsistent policy enforcement depending on group affiliation

Even if the pattern isnâ€™t across the board, youâ€™ll likely find:

- Localized or temporal bias
- Moments where severity/visibility didnâ€™t predict response, but group identity did

And thatâ€™s enough to make a powerful and specific claim.

ğŸ“ Research Isnâ€™t About Proving a Point â€” Itâ€™s About Testing One

Youâ€™re not â€œtrying to show biasâ€ â€” youâ€™re trying to find out whether bias exists under defined, observable conditions.

If you do that transparently, then whether your conclusion is yes or no, your work is:

- Credible
- Useful
- Replicable

And that makes it powerful.

---

### ğŸ§© Table 1: Incident Evaluation Pipeline

**Inclusion Rule**  
*Defines what counts as an incident*  
â†’ Neutral, identity-agnostic

**Keyword Search**  
*Retrieves a superset of candidate articles*  
â†’ Designed to surface events likely to match rule

**Screening by Rule**  
*Filters keyword results using defined criteria in the inclusion rule*  
â†’ Apply consistently â€” group/outcome blind

**Logging**  
*Track both included and excluded articles with reasons for transparency*  
â†’ Maintain transparency and repeatability

**Structured Coding**  
*Assign rule-based fields (e.g., group, severity, policy, response)*  
â†’ Enables categorical comparison across incidents

**Qualitative Coding**  
*Apply interpretive rubrics to capture tone, framing, or narrative position*  
â†’ Adds context and nuance beyond numeric fields

**Consistency Checks**  
*Test and refine coding for replicability across all incidents*  
â†’ Apply to both structured and qualitative variables

**Controlled Comparison**  
*Analyze disparities while holding severity, visibility, and legality constant*  
â†’ Reveals potential group-based bias in admin behavior

### ğŸ§¬ Table 2: Variable Types

| Type                   | Description                                               | Examples                                                              |
|------------------------|-----------------------------------------------------------|-----------------------------------------------------------------------|
| **Structured Attributes** | Rule-based, consistent, and quantifiable. These variables are coded using explicit criteria, allowing categorical comparison. | `Target_Group`, `Severity_Score`, `Policy_Broken`, `Media_Coverage_Level`, `Admin_Response` |
| **Qualitative Variables** | Interpretive but systematic. These capture nuance (e.g., tone or framing) using defined rubrics with consistent categories. | `Tone_of_Response`, `Framing_Language`, `Narrative_Positioning`, `Latency_Tone`, `Follow_Up_Action` |

### â±ï¸ Table 3: Temporal Analysis Integration

| Tool                          | Use                                                         |
|-------------------------------|--------------------------------------------------------------|
| `Date_of_Incident`, `Date_of_Response` | Calculate latency, map timelines                             |
| `Academic_Term`, `Policy_Epoch`       | Compare behavior pre-/post-major events (design choice, maybe)                     |
| Time-windowed analysis                | Detect episodic or event-specific bias                       |
| Visual tools                          | Reveal clusters, escalation patterns, or administrative silences |

### ğŸ—‚ï¸ Table 4: Data Organization Structure

**âœ… Master Incident List**  
*One row per incident*  
â†’ Includes date, location, structured and qualitative fields  
â†’ References source(s) used via `Source_IDs`  
â†’ Chronologically ordered and searchable  

**âœ… Source Appendix**  
*One row per unique source*  
â†’ Includes `Source_ID`, title, date, type (e.g., Daily Bruin, admin email)  
â†’ Describes how the source was used  
â†’ Linked to incidents via shared IDs  

**Why This Works:**  
- **Full auditability** â€” Every data point is traceable to source  
- **Traceable logic** â€” Clear chain from event â†’ coding â†’ source  
- **Separation of concerns** â€” Incidents are cleanly separated from interpretations

### Getting started 
#### âœ… Core Fields to Code From the Start
These are foundational â€” you need them early to build structured comparisons:

Format tables that separate DVs from other variables, and group structured and qualitative 

- `Incident_ID`
- `Date_of_Incident`
- `Target_Group`
- `Severity_Score`
- `Admin_Response` (Y/N)
- `Media_Coverage_Level`
- `Source_ID(s)`

#### ğŸ†” Incident ID (Incident_ID) format

- Unique identifier for each incident
- One row per incident in the **Master Incident List**
- Format: INC-001, INC-002, etc.

#### ğŸ“ Source ID (Source_ID) format 

- Unique identifier for each source document (article, email, post, etc.)
- One row per source in the **Source Appendix**
- Format: DB-001 (Daily Bruin), ADM-003 (Admin comm), SOC-005 (Social media)

#### âœ… Add-On Fields You Can Layer In Later

You donâ€™t need to lock these in up front â€” just keep them in mind:

- `Tone_of_Response` (once you've collected admin statements)
- `Latency` (once you have both date fields)
- `Policy_Epoch` or `Proximity_to_Event` (if needed after early patterns emerge)
- `Follow_Up_Action`, `Framing_Language` (after you see enough variation)

---

### ğŸ§± Staying Grounded: Avoiding Outside Inference and Project Creep

#### âœ… What â€œno outside inference requiredâ€ means:
- Stick to **what your sources say**, not assumptions or extrapolations.
- Use only your **pre-defined source set** (e.g., Daily Bruin, admin statements, lawsuits).
- Apply your **inclusion rule and coding rubrics** strictly â€” no subjective reinterpretation.


#### âš ï¸ Project Creep Risks:
- Letting in **new or inconsistent source types** mid-way (e.g., random social media or niche blogs).
- Expanding beyond your **defined time frame, incident criteria, or coding structure**.
- Rewriting your rubrics to fit difficult edge cases instead of flagging them.
- Gradually shifting from **testing a hypothesis** to **arguing a conclusion**.

#### ğŸ›¡ï¸ Guardrails to Prevent Drift

- Maintain a **locked source list** â€” document any additions as exceptions.
- If new source types are introduced for coding (e.g., admin Slack messages), **log them** in the Source Appendix and **flag as non-primary**.
- If you admit a new source type, **apply it retroactively to all relevant incidents**, not just one.
- Record any scope expansion as an **explicit methodological note** â€” never as a silent change.


### ğŸ§¾ Source vs. Source Type

**ğŸ§¾ Source = A specific document or artifact**  
An individual item you cite or use to code an incident.  
**Examples:**
- A single Daily Bruin article  
- A specific admin email sent on Oct 10  
- One Instagram post by SJP  
- A particular lawsuit filing


**ğŸ—‚ï¸ Source Type = A category of source**  
A class of materials that you allow into your dataset.  
**Examples:**
- Daily Bruin article  
- Admin public statement  
- Lawsuit filing  
- Social media post by student orgs  
- UCOP systemwide memos


### âœ… Why This Matters for Your Methodology

- You can admit **new sources** all the time (e.g., new DB articles).
- But admitting a **new source type** (e.g., Reddit threads, leaked Slack messages) is a **methodological shift**:
  - â†’ It expands the range of what you allow into your coding process.
  - â†’ So it requires **scope control**: logging, flagging as non-primary, and applying retroactively to all relevant incidents.

---

### ğŸ§­ Choosing Your Next Step

The best next step depends on your immediate goal: refining your process vs. scaling your dataset.

### âœ… Option 1: Start with a Few Candidate Incidents
*Best if your goal is to test and refine your pipeline*

Walk 2â€“3 candidate incidents through the **entire evaluation pipeline**:

- Helps you get familiar with:
  - File and folder structure
  - `Incident_ID` and `Source_ID` formatting
  - Field-by-field coding (structured + qualitative)
  - Logging excluded sources or edge cases
- Reveals ambiguities in:
  - Your inclusion rule
  - Severity or tone coding
  - Source tagging conventions

ğŸ” This approach reduces rework later and ensures your system holds up under real examples.



### âœ… Option 2: Design Your Inclusion Rule and Scrape Keywords
*Best if your goal is to begin scaling up the dataset*

- Finalize your **inclusion criteria** in plain language
- Design your **keyword search** to reliably surface relevant incidents from the Daily Bruin
- Begin your **master incident list** using this structured discovery process

ğŸ§± This sets the foundation for consistent data gathering and prevents selection bias.



### ğŸ§  Suggested Hybrid Approach

Do **one full test incident** first, end-to-end (cherry-picked is fine):
- Apply your draft **inclusion rule**
- Apply initial **structured coding rules** and **qualitative rubrics**
- Link and log all relevant sources
- Use this to test:
  - File and folder structure
  - ID formatting (`Incident_ID`, `Source_ID`)
  - Whether field definitions and coding logic are practical

 Example---Field: Severity_Score

- Possible values and definitions:

	- Low: Verbal exchange, minimal disruption, no formal complaint
	- Moderate: Property damage, threats, formal report, but no injuries
	- High: Physical altercation, hospitalization, police involvement, or major disruption to campus operations



Then, run your **inclusion rule** and draft **keyword search** on a small batch (5â€“10 real Daily Bruin articles):

- Refine your **inclusion rule**, **structured coding rules**, and **qualitative rubrics** as needed
- Adjust keyword logic based on whatâ€™s over- or under-included
- Apply the same rules/rubrics across all incidents in the batch



### âœ… When to Consider Your System Finalized

You can consider your:
- `Inclusion rule`
- `Structured coding rules`
- `Qualitative rubrics`

**Finalized** when they all hold steady across the batch â€” meaning:
- You no longer need to revise definitions mid-way
- Youâ€™re applying labels consistently and confidently
- Edge cases are being handled smoothly within your existing framework

At that point, your system is **stable**, and youâ€™re ready to scale up full incident discovery and coding with confidence.

---