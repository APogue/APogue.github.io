---
layout: outline
title: "Statistical analysis by incident"
published: true
author: Alexie Pogue
date: 2025-3-31 4:23 PM
---

I must be unbiased in this study 

### material of interest

- lawsuits

- task force reports 

- surveys

- studies

- newsroom statements

- chancellor statements

- UCOP statements

- Daily Bruin articles 

- USAC resolutions or statements 

- social media posts 

	- from the administration 

	- from student organizations 

### Create a grounded, source-agnostic incident index 

I need to build the incident list from the most neutral, time-ordered sources available

Rather than starting with lawsuits or admin comms, I'll build my incident list from the most neutral, time-ordered source available:

Best options:

- Daily Bruin: Covers both sides, timestamps all events, and youâ€™ve already tested it for keyword differences.

Build a master list of incidents first, based only on what happened and when, without regard to group or admin reaction. Only after that do you code for Target_Group, Response, etc.


### Define an incident inclusion rule (must stick to it)

To avoid subjective filtering, create a rule for what counts as an incident 

For example:

â€œAny campus-affiliated event between Sept 2023 and June 2024 where:
(a) identity-based harassment, exclusion, or violence was alleged;
(b) symbolic or physical protest occurred on university grounds; or
(c) TPM violations or safety breakdowns were documented.â€

Apply that rule consistently. Some will involve Jewish students, some Muslim/Pali â€” the rule determines inclusion, not identity or severity.

### Tag, no filters by source 

I might still find incidents via lawsuits, surveys, etc., but I won't use those to determine which ones get included. 

Instead:

- If lawsuit mentions incident â†’ tag Named_in_Lawsuit = Y

- If admin responded â†’ tag that later

- But even if they didn't â†’ it still goes in

This avoids confirmation bias.

### Supplement for representation, don't balance artificially 

I donâ€™t have to have an equal number of Jewish and Palestinian incidents â€” real-world bias might mean there are more of one kind of incident and fewer of another. What's important is:

- Your inclusion rule is neutral

- Your coding is consistent

- Your analysis accounts for coverage/severity gaps (e.g., via controls like Media_Coverage_Level)



### Methodology 

#### Defend the methodology: DB is my sole incident source

â€œThe Daily Bruin is the most comprehensive and continuously maintained public record of UCLA campus life from the student perspective.â€

âœ… 1. Itâ€™s UCLAâ€™s Student Paper of Record

- It's the only consistent, searchable, longitudinal source of campus events, student perspectives, and protest coverage over a decade.

- Administrators and students alike rely on it to understand campus dynamics.

- Unlike external media or admin comms, DB offers event-driven reporting, not top-down messaging.

â€œArticles were used to identify and time-stamp relevant incidents. No editorial interpretation from the Bruin was included in bias analysis.â€

âœ… 2. Youâ€™re Not Using It for Interpretation â€” Just Event Discovery

- You are not coding bias in their language, youâ€™re using it to identify that an incident happened on a given date, place, and with which parties involved.

- You then cross-reference the incident with admin responses, lawsuits, social media, etc.

âœ… 3. You Apply a Consistent Keyword Search and Inclusion Rule

This makes your methodology replicable and objective:

- You donâ€™t cherry-pick headlines â€” you define clear inclusion rules and search terms used across all years.

- If someone else repeated your method with the same archive, theyâ€™d get a similar incident list.

âœ… 4. You Cross-Check Admin Response Using Adminâ€™s Own Words

- The potential bias of DB is irrelevant to your main dependent variables â€” youâ€™re testing how UCLA responded, not what DB said about it.

- That makes the Bruin a cleaner baseline than admin comms, which already encode institutional motives.

ğŸ¯ Why I'm Right to Stick with the Daily Bruin

âœ… 1. Social media is not objective or complete

- Posts are ephemeral â€” deleted, edited, censored, or algorithmically buried.

- Thereâ€™s no comprehensive archive, no formal standards for coverage, no consistency in what's documented.

- It's impossible to apply a replicable incident-selection method on social media â€” you'd be relying on anecdotal, unverifiable, or engagement-biased content.

âœ… 2. DB gives you structure and timestamped reporting

- Searchable and chronologically structured

- Consistent over time (you wonâ€™t get a gap in 2017 because Instagram changed its algorithm)

- Credible enough that admin, students, and media all treat it as a public record

- You can cite specific articles, publication dates, and quotable content to ground your timeline

âœ… 3. Methodology matters more than exhaustiveness

"This isnâ€™t about cutting corners â€” itâ€™s about minimizing noise so I can precisely measure the administrationâ€™s behavior against an externally grounded record of student life."

Trying to include every mention from social media would make your study:

- Messy (inconsistent sourcing, unverifiable dates, fragmentary incidents)

- Non-replicable (no one can run the same scrape and get your dataset)

- Vulnerable to claims of cherry-picking or selective inclusion

By contrast, sticking to DB ensures:

- Clarity and transparency

- You can say: â€œEvery incident in this study was selected based on a consistent archival method from a single longitudinal source.â€
