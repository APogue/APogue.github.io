---
layout: outline
title: "Statistical analysis by incident"
published: true
author: Alexie Pogue
date: 2025-3-31 5:40 PM
---

I must be unbiased in this study 

### Material of interest

- lawsuits

- task force reports 

- surveys

- studies

- newsroom statements

- chancellor statements

- UCOP statements

- Daily Bruin articles 

- USAC resolutions or statements 

- social media posts 

	- from the administration 

	- from student organizations 

### Create a grounded, source-agnostic incident index 

I need to build the incident list from the most neutral, time-ordered sources available

Rather than starting with lawsuits or admin comms, I'll build my incident list from the most neutral, time-ordered source available:

Best options:

- Daily Bruin: Covers both sides, timestamps all events, and youâ€™ve already tested it for keyword differences.

Build a master list of incidents first, based only on what happened and when, without regard to group or admin reaction. Only after that do you code for Target_Group, Response, etc.


### Define an incident inclusion rule (must stick to it)

To avoid subjective filtering, create a rule for what counts as an incident 

For example:

â€œAny campus-affiliated event between Sept 2023 and June 2024 where:
(a) identity-based harassment, exclusion, or violence was alleged;
(b) symbolic or physical protest occurred on university grounds; or
(c) TPM violations or safety breakdowns were documented.â€

Apply that rule consistently. Some will involve Jewish students, some Muslim/Pali â€” the rule determines inclusion, not identity or severity.

### Tag, no filters by source 

I might still find incidents via lawsuits, surveys, etc., but I won't use those to determine which ones get included. 

Instead:

- If lawsuit mentions incident, admin responded â†’ tag that later

- But even if they didn't â†’ it still goes in

This avoids confirmation bias.

### Supplement for representation, don't balance artificially 

I donâ€™t have to have an equal number of Jewish and Palestinian incidents â€” real-world bias might mean there are more of one kind of incident and fewer of another. What's important is:

- Your inclusion rule is neutral

- Your coding is consistent

- Your analysis accounts for coverage/severity gaps (e.g., via controls like Media_Coverage_Level)

	- to "account for gaps," adjust for confounding variables like Severity_Score, Media_Coverage_Level, Policy_Broken

â€œDo incidents involving Palestinian students receive less administrative response than those involving Jewish students even when the severity and visibility are the same?â€

â€œAmong incidents with equal media coverage and equal severity, does the administration still respond differently depending on which group is affected?â€

If I find a consistent disparity after controlling for those other factors â†’ thatâ€™s evidence of bias.

â€œAccount forâ€ = donâ€™t ignore the fact that Jewish incidents might look different on paper â€” control for that so your findings reflect bias, not circumstance.

But,â€œIf both the media and the administration are biased â€” how can I isolate administrative bias without just proving they respond to media pressure?â€

If admin responses correlate with media coverage, it could mean:

- Admins are just responding to whatâ€™s visible (neutral media-following behavior), or

- Admins are biased like the media (shared worldview), or

- Admins are biased independent of the media (selective attention within similarly covered events)

Youâ€™re testing whether the pattern of behavior is systematically unequal even after accounting for neutral factors like visibility or severity.

ğŸ§± Use Media Coverage as a Covariate, Not a Shield

Youâ€™re not saying media coverage justifies admin action. Youâ€™re testing whether media coverage explains the action â€” and whether it explains all of it.

- If it does: the admin is reactive.

- If it doesnâ€™t: the admin is selectively reactive â€” thatâ€™s bias.

ğŸ” Lean Into Transparency

Youâ€™ll strengthen your work by saying:

â€œMedia coverage does influence administrative behavior â€” but we tested whether that explains all the disparity, and it doesnâ€™t.â€

- Youâ€™re not denying correlation.

- Youâ€™re saying correlation alone doesnâ€™t account for the patterned disparity.

- Be clear that youâ€™re not proving causation â€” youâ€™re showing systematic disparities in treatment that canâ€™t be explained by neutral factors alone

### Methodology 

#### What is this research

In research terms, youâ€™re working in the realm of:

- Qualitative Comparative Analysis (QCA)

- Case-based bias detection

- Institutional behavior analysis

- Policy response comparison

Given your project is:

- Investigating bias using incident-level comparison

- With controlled coding of metadata

- And qualitative variables like tone, latency, and severity

Using categories like Yes/Strong for bias detection patterns is a well-established approach in policy analysis, institutional review, and legal-impact studies.

As long as:

- You define your coding scheme explicitly

- You apply it consistently

- You distinguish it from statistical inference

#### Defend the methodology: DB is my sole incident source

â€œThe Daily Bruin is the most comprehensive and continuously maintained public record of UCLA campus life from the student perspective.â€

âœ… 1. Itâ€™s UCLAâ€™s Student Paper of Record

- It's the only consistent, searchable, longitudinal source of campus events, student perspectives, and protest coverage over a decade.

- Administrators and students alike rely on it to understand campus dynamics.

- Unlike external media or admin comms, DB offers event-driven reporting, not top-down messaging.

â€œArticles were used to identify and time-stamp relevant incidents. No editorial interpretation from the Bruin was included in bias analysis.â€

âœ… 2. Youâ€™re Not Using It for Interpretation â€” Just Event Discovery

- You are not coding bias in their language, youâ€™re using it to identify that an incident happened on a given date, place, and with which parties involved.

- You then cross-reference the incident with admin responses, lawsuits, social media, etc.

âœ… 3. You Apply a Consistent Keyword Search and Inclusion Rule

This makes your methodology replicable and objective:

- You donâ€™t cherry-pick headlines â€” you define clear inclusion rules and search terms used across all years.

- If someone else repeated your method with the same archive, theyâ€™d get a similar incident list.

âœ… 4. You Cross-Check Admin Response Using Adminâ€™s Own Words

- The potential bias of DB is irrelevant to your main dependent variables â€” youâ€™re testing how UCLA responded, not what DB said about it.

- That makes the Bruin a cleaner baseline than admin comms, which already encode institutional motives.

ğŸ¯ Why I'm Right to Stick with the Daily Bruin

âœ… 1. Social media is not objective or complete

- Posts are ephemeral â€” deleted, edited, censored, or algorithmically buried.

- Thereâ€™s no comprehensive archive, no formal standards for coverage, no consistency in what's documented.

- It's impossible to apply a replicable incident-selection method on social media â€” you'd be relying on anecdotal, unverifiable, or engagement-biased content.

âœ… 2. DB gives you structure and timestamped reporting

- Searchable and chronologically structured

- Consistent over time (you wonâ€™t get a gap in 2017 because Instagram changed its algorithm)

- Credible enough that admin, students, and media all treat it as a public record

- You can cite specific articles, publication dates, and quotable content to ground your timeline

âœ… 3. Methodology matters more than exhaustiveness

"This isnâ€™t about cutting corners â€” itâ€™s about minimizing noise so I can precisely measure the administrationâ€™s behavior against an externally grounded record of student life."

Trying to include every mention from social media would make your study:

- Messy (inconsistent sourcing, unverifiable dates, fragmentary incidents)

- Non-replicable (no one can run the same scrape and get your dataset)

- Vulnerable to claims of cherry-picking or selective inclusion

By contrast, sticking to DB ensures:

- Clarity and transparency

- You can say: â€œEvery incident in this study was selected based on a consistent archival method from a single longitudinal source.â€
