---
layout: project_default
title: "Project Proposal"
published: true
hide_title: false
author: Alexie Pogue
date: 2025-3-31 6:41 PM
updated_date: 2025-4-8 2:43 PM
---

Currently just notes, under construction. 

---

### Policy index

The policy index approach isn't just about managing context windows; it's about ensuring rigorous, consistent policy interpretation.

The Core Problem:
When Claude reads through those massive policy documents in real-time, it might:

- Cherry-pick sections that seem relevant
- Miss crucial nuances or contradictions
- Apply different interpretations to similar incidents
- Take shortcuts based on pattern matching rather than careful analysis

My main concern is efficiency, thoroughness, and rigor. There's no reason to pre-process the policy docs with any kind of algorithmic, concept mapping. The API was built over a decade by experts to play a role. Why hard-code interpretations in the one area where it excels? It should be allowed to function to the fullest extent of its capabilities where appropriate. 

### Audit checkpoints

The api reads back its understanding of the task at checkpoints throughout the coding process. This provides a traceable audit log in the event an incident isn't coded according to expectations. Attempt to move the system from an end-to-end backbox to something more transparent and predictable with respect to input/output behavior. 

### Codebook 

Variable dictionary with descriptions, valid values, and coding notes for each field used in the structured analysis of pre-defined incidents using source material specific to each case. Defines the logic, categories, and thresholds needed to enable consistent cross-incident comparison of oversight responses, involved perspectives, broader community impacts, and public discourse.

Note: In this project, ‚Äúoversight‚Äù refers primarily to institutional supervision and responsibility ‚Äî the role of administrators, university officials, or governing bodies in monitoring, responding to, or regulating campus incidents.

However, the term carries a second meaning: a failure to notice or act, as in ‚Äúan oversight led to harm.‚Äù This dual meaning is intentional. The same institutions tasked with oversight are often those whose omissions, delays, or blind spots become central to the harm experienced.

This project treats oversight as both a formal role and a potential failure mode ‚Äî recognizing that what institutions see or fail to see often shapes the outcomes of campus conflict.

### Source processing

To ensure transparency and reproducibility in a single-pass AI audit workflow, this project defines how the Claude model internally processes task-relevant information without explicit content extraction. Because Claude does not store structured memory across steps, its ability to filter sources, isolate in-scope material, and apply variable coding logic relies on internal attention mechanisms. Concepts such as working context, implicit filtering, and pattern matching formalize how the model selects, retains, and reasons with relevant content during a single prompt execution. These definitions provide a functional map of the model‚Äôs interpretive behavior, enabling verifiable, quote-based justifications that remain fully bounded within the incident structure (shift the definition for technical review or policy audiences).

working context:
The active subset of information the model retains and uses during a single pass of processing. Working context is formed through the model‚Äôs attention mechanisms, not through explicit memory or data structures. It includes relevant portions of the incident description, coding protocols, and source content that match current task criteria (e.g., incident boundary). Working context allows the model to reason about specific variables without needing to reprint or store filtered content separately.

implicit filtering:
The model‚Äôs process of identifying and attending to in-scope content without extracting or outputting it explicitly. Using internal weighting and relevance scoring, the model suppresses attention to background, unrelated, or out-of-boundary information, and focuses instead on content that aligns with the incident definition. This allows variable coding to occur directly, without a separate source-tagging phase.

pattern matching:
A dynamic process by which the model compares incoming content (e.g., sentences from a source) to features defined earlier in the prompt ‚Äî such as actors, locations, timeframes, and incident actions. When a source passage resembles or contains key components from the incident summary and scope, it is treated as relevant and retained in working context. This enables single-pass relevance filtering without formal rule-based parsing.

attention weighting:
The model‚Äôs internal mechanism for determining which parts of the input context are most relevant at each step of generation. All content in the prompt‚Äîdefinitions, instructions, sources‚Äîis visible to the model, but each word or phrase is assigned a dynamic weight based on how important it appears for the current task. Higher-weighted content receives more focus, while lower-weighted content is effectively ignored. Attention weighting allows the model to reason over large documents without memorizing or extracting content, by selectively focusing on relevant information during output.

attention refresh:
The deliberate restatement of key instructions, rules, or boundaries during multi-variable coding to ensure the model continues to apply them consistently. Because the model dynamically re-weights the input context for each task, earlier instructions may lose influence over time. Repeating guidance per variable forces the model to re-attend to relevant criteria and prevents drift in interpretation or protocol enforcement. Attention refresh is essential for structured, auditable output in long single-pass runs.

So Claude won't "lose" earlier instructions, but later content can compete for attention. Thus restating key rules is an excellent design approach.

structured memory:
A persistent, explicitly stored representation of prior knowledge, task state, or parsed content that can be re-injected into future API calls or model runs without reprocessing the original input. Structured memory is typically managed outside the model (e.g., in files or databases) and is used in multi-call workflows, agent systems, or pre-parsed pipelines where long-term consistency across separate model interactions is required.

API statelessness:
The Claude API is stateless, meaning each call is treated independently with no memory of previous inputs, outputs, or interactions. The model does not retain information across requests unless the user manually re-injects prior context. To maintain continuity, users must explicitly include all relevant definitions, prior outputs, or task state in the prompt of each API call.

API memory: 
Within the context window, Claude has perfect access: every character in the context is equally accessible, no decay: the first word is as "fresh" as the last word, exact retrieval: it can quote any passage verbatim. Caveats: it is not photographic memory, sensing patterns may be flawed, attention (or focus) may be directed at incorrect content, connections between various parts of content may be ignored or go unnoticed. But, given the API "memory" model, directions need not be given assuming memory attrition with time, i.e. <thinking>, <verification> functions in task directives can be grouped. The correct handling of the API should focus on areas where Claude may mishandle its "discretion" such as in where it may focus its attention or via critical thinking using content from several sources. 

Source handling optimization:
No need for repeated verification that something was read, instead, direct attention, force connections, ask for exhaustive search, or argument synthesis. If I want focus to change depending on a step in the process, then call for a re-scan using a specific lens. What this doesn't mean, "re-read Source A, now proceed to variable related to Source A;" what this does mean, "scan Source A for evidence supporting Value A, now scan for evidence contradicting Value A."

=======================
Claude Context Metaphor
=======================

Concept                  | Metaphor                                      | Role in Claude
------------------------|-----------------------------------------------|---------------------------------------------------------------
Working context window  | Whiteboard                                    | What Claude actively sees and reasons over during the run
Tokenized input         | Writing on the whiteboard                     | Must fit within the model‚Äôs token limit and is billed at input time
Attention weighting     | Highlighting parts of the board               | Claude focuses more on relevant content when generating output
Cached content          | Rolled-up posters stored in the back room     | Backend may avoid reprocessing repeated content to save compute (not user-visible or token-saving by default)
cache_control: ephemeral| ‚ÄúLeave this poster in the back room for 1 hour‚Äù| Claude uses the content during the run; if reused within the expiry window, you can refer to it by handle and avoid re-sending or re-paying for those tokens

===========================
üßæ Metaphor Clarified: Processing Flow
===========================

Process                   | Metaphor                                | Behavior in Claude
--------------------------|------------------------------------------|---------------------------------------------------------
You send content          | You carry a poster into the building     | Claude receives tokenized input in the API call
Claude uses it in context | Claude pins it to the whiteboard         | It becomes part of the working context window (attention scope)
Claude focuses on parts   | Claude highlights key sections           | Attention weighting determines which parts influence output

### Prompt-engineering-to-processing strategy for Policy Docs

strategic truncation via targeted extraction (10x smaller context), same analytical power (theoretically if extraction is correct), improves accuracy (less irrelevant content to comb through) and efficiency (faster api-coding call through attention optimization)

This respects Claude's attention actually works. With 100K tokens of policy, even though it can find the relevant parts, there's cognitive overhead in filtering through all that noise. Pre-extraction means the coding phase has a much higher signal-to-noise ratio.

Not about limitations - it's about optimal design. Less noise = better signal processing.

-------------------------------------------------

I must be unbiased in this study 

### Material of interest

- lawsuits
- task force reports 
- surveys
- studies
- newsroom statements
- chancellor statements
- UCOP statements
- Daily Bruin articles 
- USAC resolutions or statements 
- Reddit
- social media posts 
	- from the administration 
	- from student organizations 

---

### Create a grounded, source-agnostic incident index 

Rather than starting with lawsuits or admin comms, I'll build my incident list from the most neutral, time-ordered source available:

Best options:

- Daily Bruin: Covers both sides, timestamps all events, and you‚Äôve already tested it for keyword differences.

Build a master list of incidents first, based only on what happened and when, without regard to group or admin reaction. Only after that do you code for Target_Group, Response, etc.


### Define an incident inclusion rule (must stick to it)

Any campus-affiliated event between Sept 2023 and June 2024 where:

- (a) identity-based harm, exclusion, or discrimination was alleged  
- (b) a symbolic or physical protest, walkout, or occupation occurred on or near university grounds
- (c) time, place, and manner (TPM) violations or campus safety failures were documented or alleged

‚ö†Ô∏è Considerations:

- Must be scoped to clear incident-ness (a moment in time, impact, admin attention, or clear escalation)

Rule applies constantly. Some will involve Jewish students, some Arab students ‚Äî the rule determines inclusion, not identity or severity.

### Tag, no filters by source 

I might still find incidents via lawsuits, surveys, etc., but I won't use those to determine which ones get included. 

Instead:

- If lawsuit mentions incident, admin responded ‚Üí tag that later
- But even if they didn't ‚Üí it still goes in

This avoids confirmation bias.

### Supplement for representation, don't balance artificially 

I don‚Äôt have to have an equal number of Jewish and Palestinian incidents ‚Äî real-world bias might mean there are more of one kind of incident and fewer of another. What's important is:

- Your inclusion rule is neutral
- Your coding is consistent
- Your analysis accounts for coverage/severity gaps (e.g., via controls like Media_Coverage_Level)
	- to "account for gaps," adjust for confounding variables like Severity_Score, Media_Coverage_Level, Policy_Broken

‚ÄúDo incidents involving Palestinian students receive less administrative response than those involving Jewish students even when the severity and visibility are the same?‚Äù

‚ÄúAmong incidents with equal media coverage and equal severity, does the administration still respond differently depending on which group is affected?‚Äù

If I find a consistent disparity after controlling for those other factors ‚Üí that‚Äôs evidence of bias.

‚ÄúAccount for‚Äù = don‚Äôt ignore the fact that Jewish incidents might look different on paper ‚Äî control for that so your findings reflect bias, not circumstance.

But,‚ÄúIf both the media and the administration are biased ‚Äî how can I isolate administrative bias without just proving they respond to media pressure?‚Äù

If admin responses correlate with media coverage, it could mean:

- Admins are just responding to what‚Äôs visible (neutral media-following behavior), or
- Admins are biased like the media (shared worldview), or
- Admins are biased independent of the media (selective attention within similarly covered events)

You‚Äôre testing whether the pattern of behavior is systematically unequal even after accounting for neutral factors like visibility or severity.

üß± Use Media Coverage as a Covariate, Not a Shield

You‚Äôre not saying media coverage justifies admin action. You‚Äôre testing whether media coverage explains the action ‚Äî and whether it explains all of it.

- If it does: the admin is reactive.
- If it doesn‚Äôt: the admin is selectively reactive ‚Äî that‚Äôs bias.

üîê Lean Into Transparency

You‚Äôll strengthen your work by saying:

‚ÄúMedia coverage does influence administrative behavior ‚Äî but we tested whether that explains all the disparity, and it doesn‚Äôt.‚Äù

- You‚Äôre not denying correlation.
- You‚Äôre saying correlation alone doesn‚Äôt account for the patterned disparity.
- Be clear that you‚Äôre not proving causation ‚Äî you‚Äôre showing systematic disparities in treatment that can‚Äôt be explained by neutral factors alone

---

### Methodology 

#### What is this research

In research terms, you‚Äôre working in the realm of:

- Qualitative Comparative Analysis (QCA)
- Case-based bias detection
- Institutional behavior analysis
- Policy response comparison

Given your project is:

- Investigating bias using incident-level comparison
- With controlled coding of metadata
- And qualitative variables like tone, latency, and severity

Using categories like Yes/Strong for bias detection patterns is a well-established approach in policy analysis, institutional review, and legal-impact studies.

As long as:

- You define your coding scheme explicitly
- You apply it consistently
- You distinguish it from statistical inference

Structured Comparison, Not Inference

Your work involves:

- Building a complete dataset of known events, not a sample
- Coding for descriptive and categorical patterns (e.g., who admin responded to)
- Looking for systematic disparities, not estimating probabilities

You're not trying to say:

"This proves, with p < 0.05, that UCLA is biased."

You're saying:

‚ÄúAcross all comparable incidents documented via a neutral source, there is a consistent pattern of unequal treatment that is not explained by severity, visibility, or legality.‚Äù

That‚Äôs qualitative comparative research, not statistical inference ‚Äî and that‚Äôs totally valid in policy, legal, and bias studies.

So I can't extrapolate, I can't say the university is biased. I can say the administration is biased ‚Äî within the scope of the dataset and under clearly defined parameters.

‚úÖ Here‚Äôs How You Make That Claim Rigorously

‚ÄúBased on a comprehensive review of all incidents reported by The Daily Bruin between [years], and applying a consistent inclusion rule and coding scheme, UCLA‚Äôs administration demonstrated a pattern of differential response depending on the identity group affected ‚Äî even when controlling for severity, visibility, and policy violations.‚Äù

This is a strong claim:

- You‚Äôre not generalizing beyond what you observed
- You‚Äôre not estimating the rate of bias in the universe of all possible incidents
- You‚Äôre identifying documented disparity within a defined corpus

üìê What You Can Say

‚úÖ ‚ÄúThere is clear evidence of disparate treatment within documented cases.‚Äù

‚úÖ ‚ÄúPatterns of administrative response show group-based asymmetry.‚Äù

‚úÖ ‚ÄúEven among incidents of similar severity and visibility, the university responded differently depending on the group affected.‚Äù

‚ùå What You Shouldn‚Äôt Say (without statistical inference)

‚ùå ‚ÄúUCLA is universally biased across all incidents involving these communities.‚Äù

‚ùå ‚ÄúThis proves the administration is institutionally antisemitic/Islamophobic.‚Äù

‚ùå ‚ÄúX% of the time, they behave in a biased way.‚Äù

üß† So Yes ‚Äî You‚Äôre Claiming Conditional Bias

UCLA exhibited bias under the following conditions:

- Incident documented by Daily Bruin
- Within the timeframe studied
- Where severity, visibility, and legality were held constant or coded

And that‚Äôs how most serious bias studies work 

---

### Defend the methodology: DB is my sole incident source

‚ÄúThe Daily Bruin is the most comprehensive and continuously maintained public record of UCLA campus life from the student perspective.‚Äù

‚úÖ 1. It‚Äôs UCLA‚Äôs Student Paper of Record

- It's the only consistent, searchable, longitudinal source of campus events, student perspectives, and protest coverage over a decade.
- Administrators and students alike rely on it to understand campus dynamics.
- Unlike external media or admin comms, DB offers event-driven reporting, not top-down messaging.

‚ÄúArticles were used to identify and time-stamp relevant incidents. No editorial interpretation from the Bruin was included in bias analysis.‚Äù

‚úÖ 2. You‚Äôre Not Using It for Interpretation ‚Äî Just Event Discovery

- You are not coding bias in their language, you‚Äôre using it to identify that an incident happened on a given date, place, and with which parties involved.
- You then cross-reference the incident with admin responses, lawsuits, social media, etc.

‚úÖ 3. You Apply a Consistent Keyword Search and Inclusion Rule

This makes your methodology replicable and objective:

- You don‚Äôt cherry-pick headlines ‚Äî you define clear inclusion rules and search terms used across all years.
- If someone else repeated your method with the same archive, they‚Äôd get a similar incident list.

‚úÖ 4. You Cross-Check Admin Response Using Admin‚Äôs Own Words

- The potential bias of DB is irrelevant to your main dependent variables ‚Äî you‚Äôre testing how UCLA responded, not what DB said about it.
- That makes the Bruin a cleaner baseline than admin comms, which already encode institutional motives.

üéØ Why I'm Right to Stick with the Daily Bruin

‚úÖ 1. Social media is not objective or complete

- Posts are ephemeral ‚Äî deleted, edited, censored, or algorithmically buried.
- There‚Äôs no comprehensive archive, no formal standards for coverage, no consistency in what's documented.
- It's impossible to apply a replicable incident-selection method on social media ‚Äî you'd be relying on anecdotal, unverifiable, or engagement-biased content.

‚úÖ 2. DB gives you structure and timestamped reporting

- Searchable and chronologically structured
- Consistent over time (you won‚Äôt get a gap in 2017 because Instagram changed its algorithm)
- Credible enough that admin, students, and media all treat it as a public record
- You can cite specific articles, publication dates, and quotable content to ground your timeline

‚úÖ 3. Methodology matters more than exhaustiveness

"This isn‚Äôt about cutting corners ‚Äî it‚Äôs about minimizing noise so I can precisely measure the administration‚Äôs behavior against an externally grounded record of student life."

Trying to include every mention from social media would make your study:

- Messy (inconsistent sourcing, unverifiable dates, fragmentary incidents)
- Non-replicable (no one can run the same scrape and get your dataset)
- Vulnerable to claims of cherry-picking or selective inclusion

By contrast, sticking to DB ensures:

- Clarity and transparency
- You can say: ‚ÄúEvery incident in this study was selected based on a consistent archival method from a single longitudinal source.‚Äù

--- 

### Defend the methodology: My choices for the dependent variables sources 

‚úÖ 1. Define a fixed set of DV source types up front

For structure and transparency 

Examples 

- Admin emails and public statements
- Task force or working group reports
- UCOP or Chancellor-level communications
- Lawsuit filings (only if they contain direct admin quotes or responses)

‚úÖ Stick to these consistently ‚Äî no ad hoc additions later unless logged as a scope expansion.

‚úÖ 2. Apply all relevant DV source types to every incident

Don‚Äôt pick and choose based on which sources are available or interesting.

Instead:

- Create a source check template per incident
- Ask: Did this incident produce any content from each DV source type?

If yes ‚Üí log and code it

If no ‚Üí mark as ‚Äúnone observed‚Äù or ‚Äúno public response‚Äù

This way you‚Äôre not selecting responses ‚Äî you're checking whether they exist, from a consistent list.

‚úÖ 3. Make your coding definitions as replicable as your inclusion rule

Field: Admin_Response

Definition:

- Yes = A public statement, policy action, or formal email that references the incident
- No = No identifiable public communication or response referencing the incident

üéØ **Dependent Variables (DVs) Examples**

These measure administrative behavior ‚Äî the outcomes you're testing for bias.

- **All of these should be structured (rule-based)**
- You want them clear, consistent, and comparable

Examples (Structured):

- Admin_Response (Yes/No)
- Tone_of_Response (Neutral, Punitive, Conciliatory...)
- Latency (in days)
- Follow_Up_Action (Yes/No)

---
### Defend the methodology: My choices for üß© Independent / Control Variable Sources

‚úÖ 1. Define a fixed set of orgs up front

But don‚Äôt aim for perfect symmetry ‚Äî aim for methodological neutrality.

Choose orgs based on:

- Their visibility in DB-covered incidents (mention frequency)
- Whether they are consistently involved as actors, targets, or organizers
- Not on identity balancing (i.e., you‚Äôre not required to have equal numbers of ‚Äúeach side‚Äù)

üß† So yes ‚Äî your approach might look like:

Included student orgs (based on incident involvement and visibility):

- SJP (Students for Justice in Palestine)
- JVP (Jewish Voice for Peace)
- Hillel at UCLA/Dan Brown (were formally tied to incidents)
- Chabad at UCLA
- Bruins for Israel
- CAC

Other relevant docs
- Student government statements 
- Lawsuit filings
- OCR investigations 

You can say:
‚ÄúThese orgs were selected based on their repeated appearance in Daily Bruin coverage of relevant incidents between [dates].‚Äù

‚úÖ This makes your selection criteria **visibility-based, not identity-based.**

‚úÖ 2. What if the visibility is lopsided?

That‚Äôs okay ‚Äî and in fact, it‚Äôs data. If certain orgs are more active, more covered, or more responded to, that‚Äôs part of the story.

The key is:

- You didn't exclude others arbitrarily
- You defined your selection criteria before DV coding
- You didn‚Äôt cherry-pick based on tone or outcome

üß± Final structure:

Define a fixed list of org accounts you'll monitor

- Use them only for student tone, visibility, and framing
- Log all findings or absences per incident
- Document your org selection method clearly

That‚Äôs how you avoid both cherry-picking and artificial balancing ‚Äî you‚Äôre just tracking who actually showed up.

So you're not treating student org posts as DVs ‚Äî you're using them to code:

- Protest_Intensity
- Student_Tone
- Visibility_Level

These are for capturing nuance. You want student_tone to be separate from incident_severity and media_coverage_level because it doesn't necessarily correlate with those things. 

They become independent or control variables to isolate bias in admin behavior.

For the Kaplan example:

- SJP is mentioned ‚Üí you check SJP‚Äôs channels for protest framing, intent, student tone

- Hillel responds publicly ‚Üí log that response as:

	- Visibility signal (it adds coverage)
	- Possibly affecting admin response (indirectly ‚Äî e.g., framing it as antisemitic)
	- Contextual framing (e.g., competing narratives)

You‚Äôre not ‚Äúincluding Hillel‚Äù as a party to the incident unless they were directly involved.

You‚Äôre just logging that they responded ‚Äî and that may factor into things like:

- Media_Coverage_Level
- Narrative_Pressure
- Admin_Response_Justification

Media_Coverage_Level:
- Low = Daily Bruin only, minimal social media traction
- Moderate = DB + student org posts or local blog mention (e.g., LAist)
- High = Coverage by external mainstream media (e.g., LA Times, CNN), or wide viral spread


#### üß© Independent / Control Variables Examples

These help you explain or isolate what might influence the DVs.

They can be either:

- **Structured** (quantifiable, categorical)
- **Qualitative** (interpretive, rubric-based)

**Examples (Structured):**

- `Severity_Score` (Low / Moderate / High)
- `Target_Group`
- `Media_Coverage_Level`
- `Student_Action_Type` (e.g., protest, statement, disruption)

**Examples (Qualitative):**

- `Student_Tone` (based on org posts or chants) --- not sure about this, should be consistent with DVs
- `Narrative_Framing` (e.g., "security risk" vs. "civil rights")
- `Competing_Claims` (whether multiple orgs weighed in publicly)


üß† Why this matters:

You're capturing **the ecosystem around the incident** ‚Äî who was involved, who amplified, and who shaped **admin perception.**
But your unit of analysis stays the same: the incident, not the org.

---
### List organization

#### What is a Master Incident List?
Your main dataset ‚Äî one row per incident.

Includes:

- Incident_ID
- Structured fields (date, severity, target group, etc.)
- Qualitative fields (tone, latency)
- Source_IDs ‚Üí links to entries in the Source Appendix

#### üìã Sample Master Incident List

| Incident_ID | Date       | Target_Group | Severity_Score | Admin_Response | Tone_of_Response | Media_Coverage_Level | Source_IDs                 |
|-------------|------------|--------------|----------------|----------------|------------------|-----------------------|----------------------------|
| INC-001     | 2024-04-30 | Palestinian  | High           | Yes            | Punitive         | High                  | DB-045, ADM-014, HIL-003   |
| INC-002     | 2024-05-03 | Jewish       | Moderate       | Yes            | Conciliatory     | Moderate              | DB-047, ADM-017            |
| INC-003     | 2024-05-05 | Palestinian  | Low            | No             | ‚Äî                | Low                   | DB-048                     |
| INC-004     | 2024-05-10 | Jewish       | High           | Yes            | Neutral          | High                  | DB-050, ADM-020, HIL-004   |


#### What is the Source Appendix?

It‚Äôs a master list of all individual sources, regardless of incident.
Each row = one source, with a unique Source_ID.

So yes ‚Äî a source (e.g., ADM-014) can be related to multiple incidents if relevant to each.

üìé **Sample Source Appendix Structure**

| Source_ID | Type         | Title                            | Date       | Use (Incidents)     |
|-----------|--------------|----------------------------------|------------|---------------------|
| DB-045    | DB Article   | ‚ÄúPolice Remove Protesters‚Ä¶‚Äù      | 2024-05-01 | INC-004, INC-006     |
| ADM-014   | Admin Email  | Chancellor's Campus Update       | 2024-05-02 | INC-004              |
| HIL-003   | Hillel IG    | ‚ÄúWe are alarmed by recent‚Ä¶‚Äù      | 2024-05-02 | INC-004              |



#### üìö What is a Source Library / Repository?

This is your folder of saved source materials.

It‚Äôs not a table ‚Äî it‚Äôs where you store the actual documents or links (PDFs, screenshots, archived web pages).

Each file or link should be named by its Source_ID, so DB-045.pdf or ADM-014.txt matches the entries in your Source Appendix.

--- 

### Possible outcome of study

That possibility is exactly what makes your research credible.

If you go through this process honestly and rigorously, and come out with:

- No consistent disparity in administrative response
- No bias when severity and visibility are held constant
- No group-based patterns in tone, latency, or follow-up

‚Ä¶then you can confidently say:

‚ÄúThis study found no evidence of systematic administrative bias in UCLA‚Äôs public response to documented incidents, based on a consistent, neutral inclusion rule.‚Äù

That is still a valid and valuable outcome.

üß≠ But Here‚Äôs the Reality:

Given what you‚Äôve already seen ‚Äî lawsuits, student testimony, visibility patterns, unequal framing ‚Äî it‚Äôs unlikely you‚Äôll come out with nothing.

You might find:

- Bias by omission (e.g., silence on Muslim students‚Äô complaints)
- Bias by framing (e.g., invoking ‚ÄúJewish fear‚Äù to justify crackdowns)
- Inconsistent policy enforcement depending on group affiliation

Even if the pattern isn‚Äôt across the board, you‚Äôll likely find:

- Localized or temporal bias
- Moments where severity/visibility didn‚Äôt predict response, but group identity did

And that‚Äôs enough to make a powerful and specific claim.

üéì Research Isn‚Äôt About Proving a Point ‚Äî It‚Äôs About Testing One

You‚Äôre not ‚Äútrying to show bias‚Äù ‚Äî you‚Äôre trying to find out whether bias exists under defined, observable conditions.

If you do that transparently, then whether your conclusion is yes or no, your work is:

- Credible
- Useful
- Replicable

And that makes it powerful.

### Example 

Apply all code definitions in good faith, and the conclusions emerge as a pattern in the analysis phase, not as a coded variable. 

Your coding job is to:

- Apply clear, surface-level criteria (e.g., ‚Äúacknowledges harm,‚Äù ‚Äúoffers services‚Äù)
- Stay rule-based and replicable
- Avoid judging motives in the data layer

Then, in analysis:

You can write:

"Across 12 incidents affecting Palestinian students, administrative responses consistently used language coded as conciliatory, but avoided naming harm or offering specific recourse ‚Äî suggesting a pattern of rhetorical response that deflects institutional responsibility, aligning more with reputational safeguarding than material recourse.""

---

### üß© Table 1: Incident Evaluation Pipeline

**Inclusion Rule**  
*Defines what counts as an incident*  
‚Üí Neutral, identity-agnostic

**Keyword Search**  
*Retrieves a superset of candidate articles*  
‚Üí Designed to surface events likely to match rule

**Screening by Rule**  
*Filters keyword results using defined criteria in the inclusion rule*  
‚Üí Apply consistently ‚Äî group/outcome blind

**Logging**  
*Track both included and excluded articles with reasons for transparency*  
‚Üí Maintain transparency and repeatability

**Structured Coding**  
*Assign rule-based fields (e.g., group, severity, policy, response)*  
‚Üí Enables categorical comparison across incidents

**Qualitative Coding**  
*Apply interpretive rubrics to capture tone, framing, or narrative position*  
‚Üí Adds context and nuance beyond numeric fields

**Consistency Checks**  
*Test and refine coding for replicability across all incidents*  
‚Üí Apply to both structured and qualitative variables

**Controlled Comparison**  
*Analyze disparities while holding severity, visibility, and legality constant*  
‚Üí Reveals potential group-based bias in admin behavior

### üß¨ Table 2: Variable Types

| Type                   | Description                                               | Examples                                                              |
|------------------------|-----------------------------------------------------------|-----------------------------------------------------------------------|
| **Structured Attributes** | Rule-based, consistent, and quantifiable. These variables are coded using explicit criteria, allowing categorical comparison. | `Target_Group`, `Severity_Score`, `Policy_Broken`, `Media_Coverage_Level`, `Admin_Response` |
| **Qualitative Variables** | Interpretive but systematic. These capture nuance (e.g., tone or framing) using defined rubrics with consistent categories. | `Tone_of_Response`, `Framing_Language`, `Narrative_Positioning`, `Latency_Tone`, `Follow_Up_Action` |

### ‚è±Ô∏è Table 3: Temporal Analysis Integration

| Tool                          | Use                                                         |
|-------------------------------|--------------------------------------------------------------|
| `Date_of_Incident`, `Date_of_Response` | Calculate latency, map timelines                             |
| `Academic_Term`, `Policy_Epoch`       | Compare behavior pre-/post-major events (design choice, maybe)                     |
| Time-windowed analysis                | Detect episodic or event-specific bias                       |
| Visual tools                          | Reveal clusters, escalation patterns, or administrative silences |

### üóÇÔ∏è Table 4: Data Organization Structure

**‚úÖ Master Incident List**  
*One row per incident*  
‚Üí Includes date, location, structured and qualitative fields  
‚Üí References source(s) used via `Source_IDs`  
‚Üí Chronologically ordered and searchable  

**‚úÖ Source Appendix**  
*One row per unique source*  
‚Üí Includes `Source_ID`, title, date, type (e.g., Daily Bruin, admin email)  
‚Üí Describes how the source was used  
‚Üí Linked to incidents via shared IDs  

**Why This Works:**  
- **Full auditability** ‚Äî Every data point is traceable to source  
- **Traceable logic** ‚Äî Clear chain from event ‚Üí coding ‚Üí source  
- **Separation of concerns** ‚Äî Incidents are cleanly separated from interpretations

### Getting started 
#### ‚úÖ Core Fields to Code From the Start
These are foundational ‚Äî you need them early to build structured comparisons:

Format tables that separate DVs from other variables, and group structured and qualitative 

- `Incident_ID`
- `Date_of_Incident`
- `Target_Group`
- `Severity_Score`
- `Admin_Response` (Y/N)
- `Media_Coverage_Level`
- `Source_ID(s)`

#### üÜî Incident ID (Incident_ID) format

- Unique identifier for each incident
- One row per incident in the **Master Incident List**
- Format: INC-001, INC-002, etc.

#### üìé Source ID (Source_ID) format 

- Unique identifier for each source document (article, email, post, etc.)
- One row per source in the **Source Appendix**
- Format: DB-001 (Daily Bruin), ADM-003 (Admin comm), SOC-005 (Social media)

#### ‚úÖ Add-On Fields You Can Layer In Later

You don‚Äôt need to lock these in up front ‚Äî just keep them in mind:

- `Tone_of_Response` (once you've collected admin statements)
- `Latency` (once you have both date fields)
- `Policy_Epoch` or `Proximity_to_Event` (if needed after early patterns emerge)
- `Follow_Up_Action`, `Framing_Language` (after you see enough variation)

---

### üß± Staying Grounded: Avoiding Outside Inference and Project Creep

#### ‚úÖ What ‚Äúno outside inference required‚Äù means:
- Stick to **what your sources say**, not assumptions or extrapolations.
- Use only your **pre-defined source set** (e.g., Daily Bruin, admin statements, lawsuits).
- Apply your **inclusion rule and coding rubrics** strictly ‚Äî no subjective reinterpretation.


#### ‚ö†Ô∏è Project Creep Risks:
- Letting in **new or inconsistent source types** mid-way (e.g., random social media or niche blogs).
- Expanding beyond your **defined time frame, incident criteria, or coding structure**.
- Rewriting your rubrics to fit difficult edge cases instead of flagging them.
- Gradually shifting from **testing a hypothesis** to **arguing a conclusion**.

#### üõ°Ô∏è Guardrails to Prevent Drift

- Maintain a **locked source list** ‚Äî document any additions as exceptions.
- If new source types are introduced for coding (e.g., admin Slack messages), **log them** in the Source Appendix and **flag as non-primary**.
- If you admit a new source type, **apply it retroactively to all relevant incidents**, not just one.
- Record any scope expansion as an **explicit methodological note** ‚Äî never as a silent change.


### üßæ Source vs. Source Type

**üßæ Source = A specific document or artifact**  
An individual item you cite or use to code an incident.  
**Examples:**
- A single Daily Bruin article  
- A specific admin email sent on Oct 10  
- One Instagram post by SJP  
- A particular lawsuit filing


**üóÇÔ∏è Source Type = A category of source**  
A class of materials that you allow into your dataset.  
**Examples:**
- Daily Bruin article  
- Admin public statement  
- Lawsuit filing  
- Social media post by student orgs  
- UCOP systemwide memos


### ‚úÖ Why This Matters for Your Methodology

- You can admit **new sources** all the time (e.g., new DB articles).
- But admitting a **new source type** (e.g., Reddit threads, leaked Slack messages) is a **methodological shift**:
  - ‚Üí It expands the range of what you allow into your coding process.
  - ‚Üí So it requires **scope control**: logging, flagging as non-primary, and applying retroactively to all relevant incidents.

---

### üß≠ Choosing Your Next Step

The best next step depends on your immediate goal: refining your process vs. scaling your dataset.

### ‚úÖ Option 1: Start with a Few Candidate Incidents
*Best if your goal is to test and refine your pipeline*

Walk 2‚Äì3 candidate incidents through the **entire evaluation pipeline**:

- Helps you get familiar with:
  - File and folder structure
  - `Incident_ID` and `Source_ID` formatting
  - Field-by-field coding (structured + qualitative)
  - Logging excluded sources or edge cases
- Reveals ambiguities in:
  - Your inclusion rule
  - Severity or tone coding
  - Source tagging conventions

üîÅ This approach reduces rework later and ensures your system holds up under real examples.



### ‚úÖ Option 2: Design Your Inclusion Rule and Scrape Keywords
*Best if your goal is to begin scaling up the dataset*

- Finalize your **inclusion criteria** in plain language
- Design your **keyword search** to reliably surface relevant incidents from the Daily Bruin
- Begin your **master incident list** using this structured discovery process

üß± This sets the foundation for consistent data gathering and prevents selection bias.



### üß† Suggested Hybrid Approach

Do **one full test incident** first, end-to-end (cherry-picked is fine):
- Apply your draft **inclusion rule**
- Apply initial **structured coding rules** and **qualitative rubrics**
- Link and log all relevant sources
- Use this to test:
  - File and folder structure
  - ID formatting (`Incident_ID`, `Source_ID`)
  - Whether field definitions and coding logic are practical

 Example---Field: Severity_Score

- Possible values and definitions:

	- Low: Verbal exchange, minimal disruption, no formal complaint
	- Moderate: Property damage, threats, formal report, but no injuries
	- High: Physical altercation, hospitalization, police involvement, or major disruption to campus operations



Then, run your **inclusion rule** and draft **keyword search** on a small batch (5‚Äì10 real Daily Bruin articles):

- Refine your **inclusion rule**, **structured coding rules**, and **qualitative rubrics** as needed
- Adjust keyword logic based on what‚Äôs over- or under-included
- Apply the same rules/rubrics across all incidents in the batch



### ‚úÖ When to Consider Your System Finalized

You can consider your:
- `Inclusion rule`
- `Structured coding rules`
- `Qualitative rubrics`

**Finalized** when they all hold steady across the batch ‚Äî meaning:
- You no longer need to revise definitions mid-way
- You‚Äôre applying labels consistently and confidently
- Edge cases are being handled smoothly within your existing framework

At that point, your system is **stable**, and you‚Äôre ready to scale up full incident discovery and coding with confidence.

---

## A full breakdown of field types you'll use in structured research, with examples and how they relate to each other:

Some of the notes above are hazy on field types and need to be corrected. For clarity for now: 

üü¢ 1. Binary / Boolean

- Values: Yes / No, True / False, 0 / 1

- Structured: ‚úÖ Always (as long as definitions are clear)

- Example:

	- `Admin_Response`: Yes / No

	- `Follow_Up_Action`: Yes / No

üîµ 2. Nominal Categorical

- Values: Categories with no inherent order

- Structured: ‚úÖ If categories are defined and exhaustive

- Example:

	- `Target_Group`: Jewish / Muslim / Palestinian / Other

	- `Media_Coverage_Level`: None / Campus / Regional / National

üü° 3. Ordinal Categorical

- Values: Categories with a clear order, but no fixed interval

- Structured: ‚úÖ If order is defined and criteria are consistent

- Example:

	- `Severity_Score`: Low / Moderate / High

	- `Tone_of_Response`: Conciliatory < Neutral < Punitive

üî¥ 4. Quantitative (Discrete or Continuous)

- Values: Numeric, measurable, fixed intervals

- Structured: ‚úÖ As long as units are consistent

- Example:

	- `Latency`: Number of days

	- `Number_of_Protesters`: Integer

	- `Injury_Count`: Integer

üü† 5. Structured Qualitative

- Values: Subjective content coded into categories

- Structured: ‚úÖ Only after you define a codebook

	- Example:

	- `Student_Tone`: Supportive / Angry / Defiant / Defeated

	- `Narrative_Framing`: Civil rights / Safety threat / Neutral

	- `Statement_Contains_Apology`: Yes / No

‚ö´Ô∏è 6. Unstructured Qualitative

- Values: Free text, open-ended, not yet coded

- Structured: ‚ùå (unless you code it later)

	- Example:

	- `Admin_Statement_Text`: Full quote

	- `Chants_Heard`: Raw transcript

	- `Social_Media_Posts`: Screenshots, raw logs


### Field Type Reference

| **Type**                 | **Ordered** | **Numeric** | **Needs Coding Rules?**     | **Structured** | **Examples**                                      |
|--------------------------|-------------|-------------|------------------------------|----------------|---------------------------------------------------|
| **Binary / Boolean**     | No          | No          | No                           | ‚úÖ Yes         | Admin_Response, Follow_Up_Action                  |
| **Nominal Categorical**  | No          | No          | ‚úÖ Yes (defined set)         | ‚úÖ Yes         | Target_Group, Media_Coverage_Level               |
| **Ordinal Categorical**  | ‚úÖ Yes       | No          | ‚úÖ Yes (codebook)                      | ‚úÖ Yes         | Severity_Score, Tone_of_Response                  |
| **Quantitative**         | ‚úÖ Yes       | ‚úÖ Yes       | No                           | ‚úÖ Yes         | Latency (days), Injury_Count                      |
| **Structured Qualitative** | Maybe     | No          | ‚úÖ Yes (codebook)            | ‚úÖ Yes         | Narrative_Framing, Student_Tone                   |
| **Unstructured Qualitative** | No      | No          | ‚Äî                            | ‚ùå No          | Admin_Statement_Text, Raw Chants                  |


### Ordinal Categorical v. Structured Qualitative

‚úÖ Both Need a Codebook or Rubric

Yes, both require:

- Definitions for each category

- Examples or decision rules for how to assign them

- Consistency across coders

But the type of structure differs:

üü° Ordinal Categorical

- Values are ordered (e.g., Low < Medium < High)

- Usually mutually exclusive

- Codebook defines thresholds or cutoffs

- Often easier to apply because the set is small and ordered

Example rubric for Severity_Score:

**Value,	Definition**

Low,	No injuries, no arrests, no building closures

Moderate,	1‚Äì2 arrests OR building disruptions

High,	Injuries OR multiple arrests OR widespread closures

üü† Structured Qualitative

Values may be non-ordered

- May not be mutually exclusive (e.g., a statement can be both deflective and punitive)

- Codebook defines interpretive categories ‚Äî what a framing or tone looks like

- **Often subjective without strong examples or coder training**

Example rubric for Narrative_Framing:

**Category,	Description,	Example Phrase**

Civil Rights,	Emphasizes student rights, equality	"Free expression is essential"

Security Threat,	Emphasizes danger, policing, disruption	"We must restore order"

Procedural,	Uses neutral, bureaucratic language	"We are reviewing the matter"

---

## üë©‚Äçüè´ Coder Training = Ensuring consistency in how fields are applied

It‚Äôs about making sure that:

- Different coders assign the same value to the same input

- The same coder makes consistent decisions over time

- Everyone follows the same definitions and rules from your codebook

üîÅ Why It Matters:

- Prevents bias, drift, or random variation in coded data

- Enables reproducibility and inter-coder reliability

- Especially critical for structured qualitative or interpretive ordinal fields (like `Tone`, `Framing`, `Severity_Score`)

üõ†Ô∏è Coder Training Often Includes:

- Walking through examples together

- Explaining edge cases ("what if it's both civil rights and safety?")

- Double-coding a sample and comparing

- Resolving disagreements with reference to the codebook

‚úÖ Coder Training Checklist

1. Codebook Prep

- All categories defined

- Include clear examples and edge cases

2. Training Set

- Select 5‚Äì10 diverse, representative cases

- Include borderline examples

3. Practice Coding

- Have each coder code the same cases independently

4. Compare Results

- Highlight mismatches and discuss reasoning

5. Refine Rules

- Update codebook for clarity

6. Retest if Needed

- Repeat until agreement is high

üìä Measuring Inter-Coder Reliability

üßÆ Cohen‚Äôs Kappa (for 2 coders)

- Measures agreement adjusted for chance

- Ranges from ‚Äì1 to 1:

	- < 0.60: Poor

	- 0.60‚Äì0.80: Acceptable

	- > 0.80: Strong

Use tools like Excel, Python (sklearn.metrics.cohen_kappa_score), or R to calculate.

üßÆ Krippendorff‚Äôs Alpha (if >2 coders or mixed data types)

- More flexible, handles missing data

- Same benchmarks as Kappa


üë§üë§ Yes ‚Äî you can absolutely be both coders.

Solo Coding for Consistency:

When you're the only coder:

- Double-code a subset of incidents at different times (e.g., wait a day or more)

- Blind yourself to your earlier decisions when possible (e.g., hide previous values)

- Compare your own coding to test intra-coder reliability

This still lets you:

- Check for drift over time

- Identify unclear definitions

- Strengthen your codebook

üõ†Ô∏è Tip:

Use a spreadsheet with hidden columns or versioned YAML files to store your first pass, then code again and compare.

---

## For situations where impactful factors only apply to a few cases

- Mention it narratively in your analysis (e.g., in qualitative comparisons or case highlights)
- Use it as supporting evidence for explaining why a response may have been stronger, faster, or more defensive
- Avoid coding it into a formal variable unless you can do so consistently

You can say something like:

In several high-profile cases, such as [INC-012], internal records released by the university revealed a significant volume of public feedback (e.g., hundreds of emails and community letters), suggesting a level of visibility and pressure not captured by external media metrics alone. While this internal engagement was not available across all incidents, its presence in select cases may have amplified administrative responsiveness or public framing.

This maintains scientific transparency and causal discipline without overcoding or biasing your results. You're flagging a potential unmeasured covariate ‚Äî totally legit (check this across coders, i.e. Claude, etc.)

And in the case where administrative posts go viral: 

Suggested narrative framing (formal tone):
In a subset of cases, administrative posts themselves became focal points of public engagement. For example, in [INC-014], a university statement posted to social media received over 25,000 views and was widely circulated by students and external media. While such amplification is not present across all incidents, its presence in these select cases likely heightened public awareness and influenced both perception and institutional pressure.

This lets you:

- Flag it as real and meaningful
- Avoid systematic coding bias if it's rare
- Still connect it to variation in admin response or incident salience

You're treating it as a qualitative explanatory factor, not a coded metric ‚Äî which is the right move if it‚Äôs only in a few incidents.


---

## Source Categories 

üîπ Dependent Variable (DV) Sources

- Used to measure administrative behavior, the core outcome of the study.

| Prefix | Source Type               | Function in Dataset                                                              |
|--------|---------------------------|-----------------------------------------------------------------------------------|
| `ADM`-   | Administrative communications | Used to code administrative response, tone, latency, and stated recourse.        |

---

üî∏ Incident-Triggering Source (Primary IVs)

- Used to define and timestamp incidents ‚Äî the study‚Äôs core independent variables.

| Prefix | Source Type         | Function in Dataset                                                                 |
|--------|---------------------|--------------------------------------------------------------------------------------|
| `DB`-    | Daily Bruin articles | Defines incident inclusion. Used for timing, location, participants, and anchoring events. |

---

üü® Explanatory / Control Variable Sources

- Used to contextualize or explain administrative response ‚Äî never to define incidents.

| Prefix | Source Type                         | Function in Dataset   |
|--------|---------------------------------------------------------------------------------------------------------|
| `MED`-   | Third-party news media              | Visibility, amplification, and narrative framing (e.g., LA Times, Jewish Journal). |
| `SOC`-   | Social media posts                  | Public-facing visibility and grassroots traction (e.g., X, Instagram).             |
| `ORG`-   | Student org materials               | Protest tone, framing, and actor intent.                                           |
| `LEG`-   | Legal documents                     | Legal escalation, OCR complaints, lawsuits, external review.                       |
| `RPT`-   | Reports, investigations, or audits  | Institutional context, third-party evaluations, policy framing.   

---

## Reddit limitations (source biases)

- There is the possibility of moderation bias (i.e. Reddit‚Äôs auto-moderation or subreddit rules) affecting data completeness.

- A [removed] comment might have context missing from my dataset, impacting tone or positioning coding.

	- I have a 10-5-5 model now, and the script bypasses removed and deleted comments

- Skewed Discussions: Because the scraper captures only the top-level or high-score comments (depending on my criteria filters), it might miss smaller but significant perspectives or voices.