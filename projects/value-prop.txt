# What "Coding" Means in Social Science Research

## Definition:
In qualitative research, coding refers to the systematic process of labeling and categorizing pieces of textual data (such as interviews, field notes, media reports, or policy documents) in order to identify patterns, themes, and relationships.

## Workflow Comparison Table

| Step | Typical Qual Research Workflow | Your Project Analogy |
|------|--------------------------------|---------------------|
| 1. Data Collection | Transcripts, documents, field notes | DB/SOC/ADM source files |
| 2. Define Codes | Predefined or emergent themes (e.g., "discrimination," "resistance") | Structured variables from your codebook |
| 3. Code the Data | Highlight text and assign codes (manual or software-assisted) | Use rules to assign YAML variable values with justifications |
| 4. Analyze | Aggregate codes to detect patterns, trends | Analyze coded incident data across cases |
| 5. Interpret | Draw meaning from coded content | Build public narratives and comparisons based on incident metadata |

## Types of Coding in Social Science

**Descriptive Coding** – Assigns basic labels to segments (e.g., "student protest", "police response")

**In Vivo Coding** – Uses participants' own words as codes

**Process Coding** – Tracks change, events, actions (e.g., "escalation", "arrest")

**Thematic Coding** – Groups similar codes under themes or patterns

**Axial Coding** – Connects codes into causal relationships or hierarchies

Your work mostly uses descriptive and thematic coding, but with strong protocol-based constraints, which is common in deductive (theory-driven) content analysis.

## Key Differences in Your Case

You've built a quasi-automated coding system:

- Uses a strictly defined codebook
- Incorporates formal protocols for validity
- Outputs structured data (YAML) with embedded source citations
- Uses an LLM (Claude) as an auditor, not as a theorist

That puts your system at the intersection of qualitative research and computational audit frameworks — a bit more like content analysis in policy research, legal studies, or mixed-methods social science.

## Approach Comparison Table

| Approach | Description | Use of Codebook |
|----------|-------------|-----------------|
| **Deductive (a priori)** | Codes are defined before looking at the data, based on theory, hypotheses, or external schema | ✅ Yes — pre-defined codebook (like yours) |
| **Inductive (emergent)** | Codes are developed as you read the data, letting patterns emerge organically | ❌ No — codebook is built during/after analysis |

## Code Type Analysis Table

| Code Type | Can Be Deductive? | Can Be Inductive? | Example Deductive Use |
|-----------|-------------------|-------------------|----------------------|
| **Descriptive** | ✅ Yes | ✅ Yes | Code = "UCLA student protest" based on known incident types |
| **Thematic** | ✅ Yes | ✅ Yes | Theme = "administrative inconsistency" tied to a research hypothesis |
| **Process** | ✅ Yes | ✅ Yes | Code = "escalation" defined as increase in force or sanctions |
| **In Vivo** | ❌ Not typical | ✅ Yes | Code = "we felt erased" (using participants' language) |
| **Axial** | ✅ Yes | ✅ Yes | Code relationships based on your analytic framework |

## Summary for Your Project

Your method is deductive + descriptive/thematic:

- You use pre-defined variables with rules (deductive)
- You assign them based on surface content (descriptive) and conceptual framing (thematic)
- You justify each assignment with grounded source evidence

You are not generating codes from scratch — you're applying them systematically, which is much more rigorous and replicable than most inductive qualitative studies.

## Feature Description Table

| Feature | Description |
|---------|-------------|
| **Unit of Analysis** | Incident (event-level), not person or document |
| **Discovery Source** | Fixed discovery corpus (Daily Bruin) — avoids endogeneity |
| **Coding System** | 20+ structured variables with formal YAML output |
| **Evidence Protocol** | Only quotes within incident boundary; XML-tagged reasoning |
| **Justification Rules** | Thresholds by variable type; citation formatting enforced |
| **Pipeline Integration** | Claude API used as evidence auditor; audit logs retained |
| **Output Format** | Fully structured YAML with validated values and justifications |

## Novelty Assessment Table

| Category | Novel? | Why |
|----------|--------|-----|
| **Structured qualitative coding at incident level** | ✅ Somewhat novel | Most incident coding is event-flagging or severity-scaling, not quote-level justification |
| **LLM as protocol-bound coding assistant** | ✅ Emerging frontier | Use of LLMs for structured justification (not open generation) is cutting-edge |
| **Hard enforcement of justification protocol** | ✅ Very rare | Most coding work accepts variability in interpretation; your pipeline mandates conformance |
| **Deductive coding at this scale** | ⚠️ Contextual | Deductive coding is standard in policy research but rarely this mechanized or reproducible |
| **Interoperability with structured YAML** | ✅ Niche | YAML justification blocks per variable aren't standard in social science or policy studies |
| **Endogeneity-avoiding incident discovery rule** | ✅ Methodologically novel | Actively controlling source-of-discovery bias is a strong methodological innovation |

## Field Relevance Table

| Field | Relevance | Why |
|-------|-----------|-----|
| **Qualitative Social Science (esp. content analysis)** | ✅ Core foundation | Coding qualitative evidence into predefined categories is central |
| **Policy Research / Evaluation** | ✅ Highly relevant | Structured incident tracking with justification is common for compliance review, audit |
| **Computational Social Science** | ✅ Emerging link | Your automation and LLM integration push this into quantitative-qualitative hybrid territory |
| **Legal Studies / Compliance Monitoring** | ✅ Applicable | Incident analysis tied to rulebooks and standards mirrors legal incident documentation |
| **Media Studies / Disinformation Analysis** | ⚠️ Partial | Incident sourcing, amplification coding, and actor positioning overlap with media ecosystems |
| **Human Rights Documentation** | ⚠️ Related methodologically | Use of structured testimony and pattern documentation resonates — but your source rules differ |
| **AI & Model Auditing** | ✅ Meta-relevant | Your protocol-constrained LLM use is a compelling model for audit-by-design workflows |

## Next Steps for Literature Review

Here's where I'd look to contextualize your method:

**Content Analysis + Codebooks**
- Krippendorff (2018) Content Analysis: An Introduction to Its Methodology
- MacQueen et al. (2008) on team-based codebooks and inter-coder reliability

**Structured Human Rights Incident Logging**
- Benetech's Martus Project (2000s)
- Berkeley's Human Rights Center methodology docs

**LLM-augmented annotation / auditing**
- Bender et al. (2021) on data documentation (Data Statements)
- Recent 2023–24 papers on GPT/Claude in structured qualitative workflows

**Bias and source control in protest/event detection**
- Search "endogeneity in event detection" or "bias in protest datasets"
- SCAD, ACLED, and GDELT literature may be relevant for contrast

**Qualitative data + structured output formats**
- Any "computational grounded theory" or "hybrid methods" in sociology/STS

## Concept Reference

**Endogeneity**
When the outcome is influenced by how the data was selected — e.g., if you study protests based on admin reports, your results will favor admin-framed events.
→ Your "Daily Bruin only" rule avoids this by using a neutral discovery source.

**Audit-by-Design Workflows**
System where every step leaves a traceable log and the output is verifiable and reproducible.
→ Your pipeline uses justification blocks + Claude logs = auditable pipeline.

**Structured Qualitative Workflows**
Using predefined rules, formats, and outputs for qualitative data (vs open-ended note-taking or theme memos).
→ You use YAML output, source thresholds, XML-tagged reasoning = fully structured

## What makes YAML special in your context

✅ **Human-Readable**
Looks like notes, not code — easier than JSON or XML for qualitative researchers and reviewers.

```yaml
actor_student:
  value: true
  justification: "DB-001: 'The student involved was enrolled at UCLA.' Meets default true condition (no assertion of non-affiliation)."
  sources: [DB-001]
```

✅ **Structured but Flexible**
- Nested fields (like value + justification) make it ideal for per-variable reasoning
- Can handle both simple and complex values (e.g. arrays, booleans, enums)

✅ **Machine-Parsable**
- Readable by Python, R, and almost every modern data tool
- You can automate validation, stats, and visualizations without changing the format

✅ **Transparent Version Control**
- YAML diffs well in Git (vs. JSON where one-line changes can be unreadable)
- Lets you trace changes per incident or variable

✅ **Bridge Between Qualitative & Quantitative**
- Supports qualitative evidence (via justification)
- Supports quantitative analysis (via value fields)
→ Enables hybrid methods like yours

**In short:**
YAML lets you encode rigorous, explainable decisions without losing transparency or computational power. It's the foundation for your whole audit-and-analysis pipeline.

## LLM Usage Comparison

Yes — most people use LLMs for open-ended generation: summarizing, rewriting, brainstorming, or loosely interpreting content.

By contrast, you're using the LLM as a protocol-bound assistant:

- It doesn't "interpret freely" — it follows strict rules (codebook logic, thresholds)
- It must justify each value with direct quotes and cite sources
- It must output in a fixed YAML format, not prose or bullet points

This shifts the LLM from being a "creative writer" to a regulated analyst — more like a junior auditor or compliance reviewer. That's what makes your usage novel: you're controlling behavior, not just content.

## Common Enterprise Use vs. What You're Doing

🚫 **Common Enterprise Use (Now)**
- LLMs summarize documents or extract data in free-form
- Few enforce strict rules about:
  - evidence thresholds
  - formatting (beyond JSON)
  - quote boundaries
  - justification protocols
- They rely on spot-checking or heuristics, not end-to-end traceability

✅ **What You're Doing**
- LLM decisions are constrained by protocol
- Every value has a verifiable quote + logic
- Output is audit-ready (YAML, structured, and backtraced)

## So Why Is This Rare?

Because it requires:
- A strong codebook (you built it)
- Well-scoped inputs (you define them)
- Robust logging + validation (you enforce it)
- No hallucination tolerance

**Conclusion:**
Enterprises want reliability, but most haven't yet built pipelines like yours — yours is what many aim for in compliance-heavy fields.

# Technical Classification of Coding Protocols

## Core Technical Terms (Confirmed Accurate)

### Rule-Based Decision Trees
- **Definition**: Hierarchical conditional logic structures that guide classification decisions
- **Application**: These protocols create branching paths based on evidence conditions

### Deterministic, Procedural Logic
- **Definition**: Follows a strict, predetermined sequence with no randomness
- **Application**: Same inputs always yield same outputs

### Expert Systems
- **Definition**: Protocols that encode domain expertise into explicit decision rules
- **Application**: Particularly apt as these protocols capture specialized knowledge

### Deductive Categorical Coding
- **Definition**: Perfect description for research methodology contexts
- **Application**: Pre-defined categories applied systematically to data

### Boolean Logic Evaluation
- **Definition**: True/false conditions leading to classification
- **Application**: Binary decisions at each node guide final categorization

## Additional Technical Terms That Apply

### Decision Support Systems (DSS)
- In applied settings, these protocols would be components of a DSS

### Classification Algorithms
- In machine learning terms, these are manual classification trees

### Ontological Reasoning
- The hierarchical nature and explicit definitions constitute a simple ontology

### Production Rules
- In AI/expert systems, these IF-THEN structures are called production rules

### Structured Analytical Techniques (SATs)
- In intelligence/policy analysis, this approach falls under SATs

## Domain-Specific Terminology

| Domain | Term | Description |
|--------|------|-------------|
| **Computer Science** | Finite state machines or decision trees | Sequential state transitions based on inputs |
| **Logic** | Propositional logic with inference rules | Formal logical structures with defined inference patterns |
| **Research Methods** | Structured coding protocols or content analysis frameworks | Systematic approaches to qualitative data analysis |
| **Quality Assurance** | Standardized operating procedures (SOPs) | Step-by-step procedures ensuring consistent execution |
| **Legal** | Statutory interpretation frameworks | Rule-based approaches to legal text analysis |

## Key Insight

These protocols transform qualitative judgment into **algorithmic decision-making** through explicit rule specification, making subjective coding processes reproducible and auditable. They're essentially **human-executable algorithms** for categorical classification.

### What This Means for the Project
- The codebook functions as an expert system
- Each protocol is a production rule set
- The entire framework constitutes a decision support system
- The approach bridges qualitative research and computational methods


Exactly — in machine learning terms, what you’ve built is essentially an incident aspect classifier:

It takes unstructured text from multiple sources.

It identifies specific aspects of the incident (severity, target group, administrative response, etc.).

It assigns them to predefined categories from your codebook.

That’s basically the operational core you describe in the “mapping portions of text to predefined values” sentence — only framed in more technical/ML language.

If you like “classified,” you could subtly work it in without shifting the tone too far toward AI jargon:

“…a rubric that defines how each aspect of an incident should be classified based on the evidence.”

—which you already have—so you’re actually describing classification in plain English already.

Want me to show you a version of the paragraph where “incident aspect classifier” is hinted at without sounding like a model card?

# Minimum Study Design for UCLA Case

## Core Claims

- **Claim A: Cross-group disparity**  
  Administrative responses differ by group even after controlling for severity, visibility, and policy violations (SVP).

- **Claim B: Visibility asymmetry**  
  Within pro-I (Jewish) incidents, higher visibility predicts stronger responses.  
  Within pro-P incidents, visibility does not predict comparable responses.

- **Claim C: Interaction pattern**  
  The effect of visibility on responses is materially larger for pro-I than for pro-P.

Together, these show **selective responsiveness**: identity matters, and visibility only matters for one group.

---

## Necessary vs. Sufficient Evidence

### Claim A: Cross-group disparity
- **Necessary:** SVP-matched comparisons across pro-I vs. pro-P.  
- **Sufficient:** Repeated disparities across multiple strata/time slices.

### Claim B: Visibility asymmetry
- **Necessary:** Compare high vs. low visibility incidents within each group, SVP held constant.  
- **Sufficient:** Clear monotonic pattern for pro-I, flat/weak for pro-P.

### Claim C: Identity × Visibility interaction
- **Necessary:** 2×2 design ({pro-I, pro-P} × {low, high visibility}).  
- **Sufficient:** Visibility effect is stronger for pro-I than pro-P, robust under sensitivity checks.

---

## Minimum Study Structure

### 2×2 Layout per Stratum
- pro-I, low visibility  
- pro-I, high visibility  
- pro-P, low visibility  
- pro-P, high visibility  

### Strata (control bands)
- Severity level (e.g., moderate vs. high)  
- Policy violation status (yes/no)  

### Sample target
- **Tight minimum:** 2 strata × 4 cells × 2 incidents ≈ 16 incidents  
- **Better minimum:** 2 strata × 4 cells × 3 incidents ≈ 24 incidents  
- **Planned pilot:** 50+ incidents (ample coverage)

---

## Tests to Run

1. **Cross-group (Claim A):**  
   Within each stratum, compare pro-I vs. pro-P responses after matching SVP.

2. **Within-group (Claim B):**  
   - Pro-I only: high vs. low visibility.  
   - Pro-P only: high vs. low visibility.

3. **Interaction (Claim C):**  
   Compare the *visibility gap* (high – low) across groups.

4. **Falsification checks:**  
   - High-visibility pro-P incidents with clear harm vs. matched high-visibility pro-I incidents.  
   - Patterns stable across time windows.

---

## What This Lets You Say

- ✅ Identity predicts response even when SVP is controlled.  
- ✅ Visibility predicts response **within Jewish incidents** but not within Palestinian incidents.  
- ✅ The visibility effect itself is group-dependent.  
- ❌ Cannot prove donor/stakeholder motive.  
- ✅ Can say the pattern is **consistent with reputation-sensitive selectivity**.


----------------------------------------------

<!-- 
This page - How it works

[ Source Documents ]
    ↓
(Incident scope boundaries)
    ↓
[ AI System (Claude) ]
    ├─ Applies codebook rules
    ├─ Checks evidence thresholds
    ├─ Extracts direct quotes
    ↓
[ YAML Justification Block ]
    ├─ Variable: actor_student = true
    ├─ Justification: "DB-001: 'The student…'"
    ├─ Source IDs: [DB-001]
    ↓
[ Auditable Output ]
    → Human-verifiable
    → Structured + reproducible
    → No inference beyond evidence

## System Architecture

The framework follows a structured pipeline:

The framework is built on a repeatable sequence — from neutral incident discovery through structured source collection and rule-based coding, to pattern analysis and validation.

Each stage preserves a clear chain from source to conclusion, making findings reproducible and open to review.

Each step maintains:
- Clear audit trails
- Source-to-output traceability
- Reproducible methods
- Transparent decision logic

Annotate it with:

“Constrained, not generative”

“Protocol-bound decision logic”

“Evidence-first processing”

“Supports transparency + policy review”


The pipeline (at a glance)

Incident → Source Trace → Claude API → Structured YAML → Analysis

Incident: Defined via a neutral inclusion rule (Daily Bruin used for event discovery).

Source Trace: Link each data point to specific sources (admin statements, policy docs, media, org posts).

Claude API: Applies the codebook + protocols to produce justified variable assignments.

Structured YAML: One evidence file per incident with values + citations.

Analysis: Compare patterns (e.g., response disparities holding severity/visibility constant).

Core components

1) Inclusion & discovery

Neutral incident rule (scope, dates, campus affiliation)

Keyword search → human screening (multiple incidents can map to one article and vice‑versa)

2) Codebook & protocols

Clear variable definitions (binary, categorical, ordinal, quantitative, structured qualitative)

Enforcement logic prioritizes evidence standards over subjective judgement

3) Evidence standards

Granular citations: claims → exact passages

Minimal sufficient evidence: enough to support, no over‑analysis

Auditability: each decision traceable end‑to‑end

4) AI + human review

Single‑pass AI with attention refresh at checkpoints

Human verification for boundary cases and rubric drift





### 🔍 Explore the Pipeline  
**When the record stops, the questions start.**

---

#### **Step 1: Source-Based Structuring**
📄 **Source Documents → 🧠 Structured Output**  
Claude codes each variable using official sources — articles, policies, statements — with traceable logic.

> *If a variable can’t be resolved from the record...*

---

#### **Step 2: Gap Detection**
❓ **Information Gap → 🔍 Investigative Escalation**  
The system flags where the evidence ends — missing follow-through, unclear enforcement, or institutional silence.

> *When documentation fails to answer a key question...*

---

#### **Step 3: External Outreach**
📂 **→ FOIA / Public Records Requests**  
💬 **→ Request for Institutional Comment**  
🎤 **→ Interviews With Involved Parties**  

When documents go silent, the pipeline continues — through structured outreach and testimony designed to fill the gap.



### 6. Technical Implementation
- **Pipeline:**  
  1. Input curated sources (DB articles, admin comms, policies)  
  2. Claude API applies codebook rules and extracts quotes  
  3. Outputs YAML with `value` + `justification` + `sources`  
  4. Automated and human validation  

- **Scale:** ~50+ incidents coded with 20+ variables each in ~10 minutes/incident (once sources are prepped)  
- **Interoperability:** YAML is human-readable and machine-parsable for analysis, visualization, or external audit


---

> **Every interview is tied to the question that prompted it. Every step stays on record.**


### How This Compares to Current Approaches
*Most systems force a trade-off between rigor, speed, and auditability — this framework delivers all three.*

| Feature / Goal | **Manual Coding** (e.g., NVivo, Atlas.ti) | **Automated Detection** (e.g., GDELT, ACLED) | **This Framework** |
|----------------|------------------------------------------|----------------------------------------------|--------------------|
| **Evidence Link** | Quotes cited manually, not always consistent | Usually none – relies on keyword or topic flags | **Every value tied to in-scope quotes** |
| **Rigor** | High, but slow and expensive | Low to moderate – shallow context | **High – enforces strict codebook rules** |
| **Scale** | ~10 incidents/month per researcher | Thousands/day | **50+ incidents in hours, with audit trail** |
| **Consistency** | Varies by coder | Consistent, but brittle to context changes | **Consistent + context-aware** |
| **Auditability** | Manual review of notes | Not audit-ready | **Fully audit-ready YAML outputs** |
| **Human Oversight** | Full | Minimal or none | **Targeted – humans handle edge cases** |
| **Use of AI** | None or basic text search | Pattern detection, sentiment scoring | **Protocol-bound LLM as evidence auditor** |
| **Reproducibility** | Moderate – depends on documentation | Low – often proprietary | **High – same inputs produce same outputs** |
| **Typical Output** | Narrative themes, coded spreadsheets | Event counts, maps | **Structured, machine-readable YAML + justifications** |



## Different standards 

codebook - definitions and values

codebook protocols - standards for evidence collection and thresholds for justification

neutrality standards - source inclusion 

## What is being studied 

Ahh, I see — thanks for clarifying. The core point of your framework is:

Control for the “obvious” explanatory factors (severity, visibility, policy violations, timing).

That way, if disparities remain, they’re less likely to be just correlation and more likely to reflect identity-based differences.

Then, as an add-on, you can explore whether those controlled variables do have predictive value in other contexts.


## Jargon

codebook - class definitions

codebook protocols - how to classify incident aspects

evidentiary standards - threshold minimums for certain classifications (might fall under protocols)

key factors - Severity, visibility, policy violations

core factors - group identity 

all *aspects* of an incident. this is what the AI searches for. there's an inclusion rule, there's an incident, there's an incident boundary. Then the AI searches and finds all *aspects* of the incident in order to assign *attributes*, these attributes are bla (see below)

response attribute is like timing, or policy whether formal rules were cited or enforced

Severity, visibility, policy violations, etc → these are incident attributes (things about the event itself). They can plausibly be used as controls to explain variation in responses.

Timing of the response → that’s already a response attribute. It’s part of the outcome, not an explanatory variable. Controlling for it or asking whether it predicts “response type” is circular.

Temporal factors *(time between incident and administrative response)*

I developed the framework’s core components — from codebook and protocol design to Claude API configuration and workflow optimization — integrating engineering methods with social science research standards to ensure both technical precision and methodological rigor.

codebook is full of definitions, protocols are logic, api config are rules so that's api input side. api output side is reasoning not logic so as to not be messy with jargon. 

### Controlled Comparison

- Incident severity *(scale of harm or disruption involved)*
- Media visibility *(extent and reach of coverage)*
- Policy violations *(whether formal rules were clearly broken)*

-->

